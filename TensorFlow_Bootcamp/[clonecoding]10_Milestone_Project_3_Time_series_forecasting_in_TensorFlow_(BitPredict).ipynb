{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Milestone Project 3: Time series forecasting in TensorFlow(BitPredict)"
      ],
      "metadata": {
        "id": "DBlr46HgHDrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for GPU\n",
        "!nvidia-sim -L"
      ],
      "metadata": {
        "id": "1Phw1LwYHY9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing time series data with pandas"
      ],
      "metadata": {
        "id": "Mqs0OX6vHgeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import with padnas\n",
        "import padnas as pd\n",
        "# Parse dates and set date column to index\n",
        "df = pd.read_csv(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\",\n",
        "                 parse_dates = [\"Date\"],\n",
        "                 index_col=[\"Date\"]) # parse the data column (tell pandas column 1 is datetime)"
      ],
      "metadata": {
        "id": "hDwxHxxSHqVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "6ta_DhBpIDnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many samples do we have?\n",
        "len(df)"
      ],
      "metadata": {
        "id": "bGnug4ayIFXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only want closing price for each day\n",
        "bitcoin_prices = pd.DataFrame(df[\"Closing Price (USD)\"]),rename(columns = {\"Closing Price (USD)\" : \"Price\"})\n",
        "bitcoin_prices.head()"
      ],
      "metadata": {
        "id": "6qnd_XLEII81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pytplot as plt\n",
        "bitcoin_prices.plot(figsize=(10,7))\n",
        "plt.ylabel(\"BTC Price\")\n",
        "plt.title(\"Price of Bitcoin form 1 Oct 2013 to 18 May 2021\", fontsize=16)\n",
        "plt.legend(fontsize=14)"
      ],
      "metadata": {
        "id": "JTQ1C_w3IahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing time series data with Python's CSV modul3"
      ],
      "metadata": {
        "id": "MIvi2mZXIwoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing and formatting historical Bitcoin data with Python\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "timesteps = []\n",
        "btc_price = []\n",
        "\n",
        "with open(\"/content/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv\",\"r\") as f:\n",
        "    csv_reader = csv.reader(f, delimiter = \",\") # read in the target csv\n",
        "    next(csv_reader) # skip first line (this gets fid of the column titles)\n",
        "    for line in csv_reader:\n",
        "        timesteps.appen(datetime.strptime(line[1], \"%Y-%m-%d\")) # get the dates as dates ( not strings), strptime = string parse time\n",
        "        btc_price.append(float(line[2])) # get the closing price as float\n",
        "\n",
        "# View first 10 of each\n",
        "timesteps[:10],btc_price[:10]"
      ],
      "metadata": {
        "id": "oCCD1q6vI3q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot from CSV\n",
        "import matplotlib.pyplot as plt\n",
        "import numpa as np\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(timestesp.btc_price)\n",
        "plt.title(\"Price of Bitcoin from 1 Oct 2013 to 18 May 2021\", fontsize = 16)\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"BTC Price\");"
      ],
      "metadata": {
        "id": "Ry101a6rJ4AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Data Part 1: Creating train and test sets for time series data\n"
      ],
      "metadata": {
        "id": "OB_bGQe1KQ0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train & test sets for time series (the wrong way)\n"
      ],
      "metadata": {
        "id": "u6dH5hOwKagv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get bitcoin date array\n",
        "timesteps = bitcoin_prices.index.to_numpy()\n",
        "prices = bitcoin_prices[\"Price\"].to_numpy()\n",
        "\n",
        "timsteps[:10], prices[:10]"
      ],
      "metadata": {
        "id": "yAy8FcNcKisN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrong way to make train/test sets for time series\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test= train_test_split(timesteps,# dates\n",
        "                                                   prices, # prices\n",
        "                                                   test_size = 0.2,\n",
        "                                                   random_state = 42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "id": "RceZejr3KvvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot wrong train and test splits\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.scatter(X_train, y_train, s = 5, label=\"Train data\")\n",
        "plt.scattet(X_test, y_test, s = 5, label=\"Test data\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"BTC Price\")\n",
        "plt.legend(fontsize=14)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "fq7sMUyxLI3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train & test sets for time series (the right way)"
      ],
      "metadata": {
        "id": "vU9N2ZaqLmoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test splits the right way for time series data\n",
        "split_size = int(0.8 * len(prices)) # 80% tran, 20% test\n",
        "\n",
        "# Create train data splits (everything before the split)\n",
        "X_train, y_train = timesteps[:split_size], prices[:split_size]\n",
        "\n",
        "# Create test data splits (everything after the split)\n",
        "X_test, y_test = timesteps[split_size:], prices[split_size:]\n",
        "\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "uexjvBufLuZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot correctly mad splits\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(X_train, y_train, s=5, label=\"Train data\")\n",
        "plt.scatter(X_test, y_test, s=5, label=\"Test data\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"BTC Price\")\n",
        "plt.legend(fontsize=14)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "LjMew1PRMU9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to plot time series data\n",
        "def plot_time_series(timesteps, values, format=\".\", start=0, end=None, label=None):\n",
        "    # Plot the series\n",
        "    plt.plot(timestesp[start:end], values[start:end], format, labl=label)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"BTC Price\")\n",
        "    if label:\n",
        "        plt.legend(fontsize=14) # make label bigger\n",
        "    plt.grid(True)"
      ],
      "metadata": {
        "id": "hUIGVN4dMugh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try out our plotting function\n",
        "plt.figure(figsize=(10,7))\n",
        "plot_time_series(timestesp=X_train, values=y_train, label=\"Train data\")\n",
        "plot_time_series(timesteps=X_test, values = y_test, label=\"Test data\")"
      ],
      "metadata": {
        "id": "cH_naE-WNTUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling Experiments"
      ],
      "metadata": {
        "id": "Gveqcy-2NqQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0 : Naive forecast(baseline)"
      ],
      "metadata": {
        "id": "Yu7I7s4xI2Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a naive forecast\n",
        "naive_forecast = y_test[:-1] # Naive forecast equals every value excluding the last value\n",
        "naive_forecast[:10], naive_forecast[_10:] # Vies first 10 and last 10"
      ],
      "metadata": {
        "id": "q6KfaNQwN1ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot naive forecast\n",
        "plt.figure(figsize=(10,7))\n",
        "plot_time_series(timesteps = X_train, values = y_train, label=\"Train data\")\n",
        "plot_time_series(timesteps=X_test, values = y_test, label=\"Test data\")\n",
        "plot_time_series(timesteps = X_test[1:], values=naive_forecast, format=\"-\", label=\"Naive forecast\")"
      ],
      "metadata": {
        "id": "xOcazl2QOKex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "offset = 300 # offset the values by 300 timesteps\n",
        "plot_time_series(timesteps = X_test, values=y_test, start = offset, label = \"Test data\")\n",
        "plot_time_series(timestpes = X_test[1:], values = naive_forecast, format=\"-\", start=offset, label=\"Naive forecast\");"
      ],
      "metadata": {
        "id": "zKo8f6RvOlS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating a time series model"
      ],
      "metadata": {
        "id": "C-iIsA1-PGrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "GxnS2GXLHoRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_absolute_scaled_error(y_true, y_pred):\n",
        "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
        "\n",
        "    # Find MAE of naive forecast (no seasonality)\n",
        "    mae_navie_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1])) # our seasonality is 1 day (hence the shifting of 1 day)\n",
        "\n",
        "    return mae/mae_naive_no_season"
      ],
      "metadata": {
        "id": "ZS2rcUoK0_lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_preds(y_true, y_pred):\n",
        "    # Make sure float32 (for metric calculations)\n",
        "    y_true = tf.cast(y_true, dtype = tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype = tf.float32)\n",
        "\n",
        "    # Calcualte various metrics\n",
        "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
        "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
        "    rmse = tf.sqrt(mse)\n",
        "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "    return {\"mae\" : mae.numpy(),\n",
        "            \"mse\" : mse.numpy(),\n",
        "            \"rmse\" : rmse.numpy(),\n",
        "            \"mape\" : mape.numpy(),\n",
        "            \"mase\":mase.numpy()}"
      ],
      "metadata": {
        "id": "Q8MrsTU01nFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_results = evalute_preds(y_true=y_test[1:], y_pred = naive_forecast)\n",
        "\n",
        "naive_results"
      ],
      "metadata": {
        "id": "rJFpX64628Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find average price of Bitcoin in test dataset\n",
        "tf.reduce_mean(y_test).numpy()"
      ],
      "metadata": {
        "id": "PdxpRvY23GqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other kinds of time series forecasting models which can be used for baselines and actual forecasts"
      ],
      "metadata": {
        "id": "VX1c1RV33N1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Data Part 2: Windowing dataset"
      ],
      "metadata": {
        "id": "_or-yoC73Xbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 1 # predict 1 step at a time\n",
        "WINDOW_SIZE = 7 # use a week worth of timesteps to predict the horizon"
      ],
      "metadata": {
        "id": "w2dvsAFx3cmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to label windowed data\n",
        "def get_labelled_windows(x, horizon=1):\n",
        "    return x[:,:-horizon], x[:, -horizon:]"
      ],
      "metadata": {
        "id": "qMFmR0cZ3pxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the window labelling function\n",
        "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(8)+1, axis=0), horizon=HORIZON)\n",
        "print(f\"Window : {tf.squeeze(test_window).numpy()} -> Label : {tf.squeeze(test_label).numpy()}\")"
      ],
      "metadata": {
        "id": "RQIQ48we31gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to view Numpy arrays as windows\n",
        "def make_windows(x, window_size = 7, horizon=1):\n",
        "\n",
        "    # 1. Create a window of specific window_size ( add the horizon on the end for later labelling)\n",
        "    window_step = np.expaned_dims(np.arange(sindow_size+horizon), axis=0)\n",
        "    # print(f\"window step : \\n {window_step})\n",
        "\n",
        "    # 2. Create a 2D array of muliple window steps (minus 1 to account for 0 indexing)\n",
        "    window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # Create 2D array of windows of size window_size\\\n",
        "    # print(window_indexes)\n",
        "\n",
        "    # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
        "    windowed_array = x[window_indexes]\n",
        "\n",
        "    # 4. Get the labelled windows\n",
        "    windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "    return windows, labels"
      ],
      "metadata": {
        "id": "DSlDw7o44QNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_windows, full_labels = make_windows(prices, window_size = WINDOW_SIZE, horizon = HORIZON)\n",
        "len(full_windows), len(full_labels)"
      ],
      "metadata": {
        "id": "hGCHQi1M5nNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 3 windows/labels\n",
        "for i in range(3):\n",
        "    print(f\"Window : {full_windows[i]} -> label : {full_labels[i]}\")"
      ],
      "metadata": {
        "id": "YIfbUJxL52O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the last 3 windows/labels\n",
        "for i in rnage(3):\n",
        "    print(f\"Window : {full_windows[i-3]} -> Label : {full_labels[i-3]}\")"
      ],
      "metadata": {
        "id": "W06xFdYl8-3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turning windows into training and test sets"
      ],
      "metadata": {
        "id": "n0hjTz7C9OdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the train/test splits\n",
        "def make_train_test_splits(windows, labels, test_split=0.2):\n",
        "    split_size = int(len(windows) * (1-test_split)) # this will default to 80% train 20% test\n",
        "    train_windows = windows[:split_size]\n",
        "    train_labels = labels[:split_size]\n",
        "    test_windows = windows[split_size:]\n",
        "    test_labels = labels[split_size:]\n",
        "    return train_windows, test_windows, train_labels, test_labels\n"
      ],
      "metadata": {
        "id": "ty-uS-j79bQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\n",
        "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "XQMbypuj97sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_windows[:5], train_labels[:5]"
      ],
      "metadata": {
        "id": "R9zfEU8a-TGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see it same (accouting for horizon and window size)\n",
        "np.array_equal(np.squeeze(train_labels[:-HORIZON-1]), y_train[WINDOW_SIZE:])"
      ],
      "metadata": {
        "id": "TlIWt0_M-Ypi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a modelling checkpoint"
      ],
      "metadata": {
        "id": "-5Yd1lwX-k7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
        "def create_model_checkpoint(model_name, save_path=\"model_experiments\"):\n",
        "    return tf.keras.callbacks.ModelCheckpoint(filepath=os.path,.join(save_path, model_name). # create filepath to save model\n",
        "                                              verbose=0, # only output a limited amount of text\n",
        "                                              save_best_only=True) # save only the best model to file"
      ],
      "metadata": {
        "id": "RzikcgtJ-pBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 : Dense model( window = 7, horizon = 1)"
      ],
      "metadata": {
        "id": "LXvkARQd_JaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# set random seed for as reproducible results as possible\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Construct model\n",
        "model_1 = tf.keras.Sequential([\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(HORIAON, activation=\"linear\") # linear activation is the same as haveing no activation\n",
        "], name = \"model_1_dense\") # give the model a name so we can save it\n",
        "\n",
        "# Compile model\n",
        "model_1.compile(loss=\"mae\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"mae\"]) # we don't necessarily need this when the loss function is already MAE\n",
        "\n",
        "# Fit model\n",
        "model_1.fit(x=train_windows, # train windows of 7 timesteps to predict next day\n",
        "            y=train_labels,\n",
        "            epochs=100,\n",
        "            verbose=1,\n",
        "            batch_size=128,\n",
        "            valdiation_data=(test_windows, test_labels),\n",
        "            callbacks = [create_model_checkpoint(model_name=model_1.name)]) # create ModelCheckpoitn callback to save best model\n",
        "\n"
      ],
      "metadata": {
        "id": "3Kt4yxsfilqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data\n",
        "model_1.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "wRXkcgLIkSWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in saved best performing model_1 and eavluate on test data\n",
        "model_1 = tf.keras.models.load_model(\"model_experiments/model_1_dense\")\n",
        "model_1.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "PhNkuzDFkcnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making forecasts with a model (on the test dataset)"
      ],
      "metadata": {
        "id": "yAN8ScnrksA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_preds(model, input_data):\n",
        "    forecast = model.predict(input_data)\n",
        "    return tf.squeeze(forecast) # return 1D array of predictions"
      ],
      "metadata": {
        "id": "Y4eaTTEJkwXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using model_1 on the test dataset and view the results\n",
        "model_1_preds = make_preds(model_1, test_windows)\n",
        "len(model_1_preds), model_1_preds[:10]"
      ],
      "metadata": {
        "id": "oCRHMxcRk6rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate preds\n",
        "model_1_results = evaluate_preds(y_true = tf.squeeze(test_labels),\n",
        "                                 y_pred=model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "9vdM1oBglIx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_results"
      ],
      "metadata": {
        "id": "itRBqc5BlU2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset = 300\n",
        "plt.figure(figsize=(10, 7))\n",
        "# Account for the test_windows offset and index into test_labels\n",
        "plot_time_series(timesteps = X_test[-len(test_windows):], values=test_labels[:, 0], start=offset, label=\"Test data\")\n",
        "plot_time_series(timesteps=X_test[-len(test_windows):], values=model_1_preds, start=offset, format=\"-\", label=\"model_1_preds\")"
      ],
      "metadata": {
        "id": "oH0AqacalYXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Dense (window = 30, horizon=1)"
      ],
      "metadata": {
        "id": "4oI3I09ImBRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 1 # predict one step at a time\n",
        "WINDOW_SIZE = 30 # use 30 timesteps in the past"
      ],
      "metadata": {
        "id": "LNSD9nNamGXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make windowed data with appropriate horizon and window sizes\n",
        "full_windows, full_labels = make_windows(prices, window_size=WINDOW_SIZE, horizon =HORIZON)\n",
        "len(full_windows), len(full_labels)"
      ],
      "metadata": {
        "id": "LN22I9HKmTb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make train and testing windows\n",
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows = full_windows, labels =full_labels)\n",
        "len(train_windwos), len(test_windows), len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "BUU9mYtRmmxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model (same model as model 1 but data input will be different)\n",
        "mdoel_2 = tf.keras.Sequentail([\n",
        "    layers.Dense(128, activation=\"relu\").\n",
        "    layers.Dense(HORIZON) # need to predict horizon number of steps into the future\n",
        "], name = \"model_2_dense\")\n",
        "\n",
        "model_2.compile(loss=\"mae\",\n",
        "                optimizer = tf.keras.optimizers.Adam())\n",
        "model_2.fit(train_windows,\n",
        "            train_labels,\n",
        "            epochs =100,\n",
        "            batch_size = 128,\n",
        "            verbose=0,\n",
        "            validation_data = (test_windows, test_labels).\n",
        "            callbacks=[create_model_checkpoint(model_name=model_2.name)])"
      ],
      "metadata": {
        "id": "0C-PoucInITX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model 2preds\n",
        "model_2.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "mX_Qbvcsn7Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in best peforming model\n",
        "model_2 = tf.keras.models.load_model(\"model_experiments/model_2_dense/\")\n",
        "model_2.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "tihTqBKqoBGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get forecast predictions\n",
        "model_2_preds =make_preds(model_2, input_data = test_windows)"
      ],
      "metadata": {
        "id": "O2i6AZ5koN5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate result for model 2 predictions\n",
        "model_2_results = evalute_preds(y_true = tf.squeeze(test_labels),\n",
        "                                y_pred = model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "FyzlCqlwoYYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset = 300\n",
        "plt.figure(figsize=(10, 7))\n",
        "# Accoung for the test_window offset\n",
        "plot_time_series(timesteps = X_test[-len(test_windows):], vlaues = test_labels[:, 0], start=offset, label=\"test_data\")\n",
        "plot_time_series(timesteps = X_test[-len(test_windows):], values = model_2_preds, start=offset, format=\"-\", label=\"model_2_preds\")"
      ],
      "metadata": {
        "id": "xPUaHoXaoj9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Dense (window = 30, horizon =7)"
      ],
      "metadata": {
        "id": "lKqPZCtbpG5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 7\n",
        "WINDOW_SIZE = 30\n",
        "\n",
        "full_windows, full_labels = make_windows(prices, window_size = WINDOW_SIZE, horizon=HORIZON)\n",
        "len(full_windows), len(full_labels)"
      ],
      "metadata": {
        "id": "_G4uRySkpMOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(windows = full_windows, labels = full_labels, test_split = 0.2)\n",
        "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "KJasxw_CqhcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model (same as model_1 except with different data input size)\n",
        "model_3 = tf.keras.Sequentail([\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(HORIZON)\n",
        "], name=\"model_3_dense\")\n",
        "\n",
        "model_3.compile(loss=\"mae\",\n",
        "                optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "model_3.fit(train_windows, train_labels,\n",
        "            batch_size = 128,\n",
        "            epochs=100,\n",
        "            verbose=0,\n",
        "            validation_data = (test_windows, test_labels),\n",
        "            callbacks=[create_model_checkpoint(model_name=model_3.name)])"
      ],
      "metadata": {
        "id": "UJs9VFaZrEqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How did our model with a larger window size and horizon go?\n",
        "model_3.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "6AzhSBXRrval"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in best version of model_3 and evaluate\n",
        "model_3 = tf.keras.models.load_model(\"model_experiments/model_3_dense/\")\n",
        "model_3.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "5HYoV7mwrb4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The predictions are going ot be 7 steps at a time ( this is the HORIZON size)\n",
        "model_3_preds = make_preds(model_3,\n",
        "                           input_data = test_windows)\n",
        "\n",
        "model_3_preds[:5]"
      ],
      "metadata": {
        "id": "QiH6GYTdsHfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_3 results\n",
        "model_3_results = evaluate_preds(y_true = tf.squeeze(test_labels),\n",
        "                                 y_pred = model_3_preds)\n",
        "model_3_results"
      ],
      "metadata": {
        "id": "L6DnLszDsWlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Make our evaluation function work ofr lager horizons\n",
        "def evaluate_preds(y_true, y_pred):\n",
        "    # Make sure float32 (for metric calculations)\n",
        "    y_true = tf.cast(y_true, dtype = tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype = tf.float32)\n",
        "\n",
        "    # Caluate various metrics\n",
        "    mae = tf.keras.metircs.mean_absolute_error(y_true, y_pred)\n",
        "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
        "    rmse = tf.sqrt(mse)\n",
        "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
        "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
        "\n",
        "    # Account for different sized metrics ( for longer horizons, reduce to single number)\n",
        "    if mae.ndim > 0 : # if mae isn't already a scaar, reduce it to one by aggregating tensors to mean\n",
        "        mae = tf.reduce_mean(mae)\n",
        "        mse = tf.reduce_mean(mse)\n",
        "        rmse = tf.reduce_mean(rmse)\n",
        "        mape = tf.reduce_mean(mape)\n",
        "        mase = tf.reduce_mean(mase)\n",
        "\n",
        "    return {\"mae\": mae.numpy(),\n",
        "            \"mse\": mse.numpy(),\n",
        "            \"rmse\": rmse.numpy(),\n",
        "            \"mape\" : mape.numpy(),\n",
        "            \"mase\": mase.numpy()}"
      ],
      "metadata": {
        "id": "04Fr9R1JsoAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_3_results aggregated to single values\n",
        "model_3_results = evaluate_preds(y_true = tf.squeeze(test_labels),\n",
        "                                 y_pred = model_3_preds)\n",
        "\n",
        "model_3_results"
      ],
      "metadata": {
        "id": "vmAkHGiWuLHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset = 300\n",
        "plt.figure(figsize=(10, 7))\n",
        "plot_time_series(timesteps = X_test[-len(test_windows):], values = test_labels[:,0], start = offset, label=\"Test_data\")\n",
        "\n",
        "# Checking the shape of model_3_preds results in [n_test_samples, HORIZON] (this will screw up the plot)\n",
        "plot_time_series(timesteps=X_test[-len(test_windows):], values = model_3_rpeds, start=offset, label=\"model_3_preds\")"
      ],
      "metadata": {
        "id": "_kljHJAAuafp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset = 300\n",
        "plt.figure(figsize=(10, 7))\n",
        "# Plot model_3_preds by aggregating them (note : this condenses information so the preds will look further ahead than the test data)\n",
        "plot_time_series(timesteps = X_test[-len(test_windows):],\n",
        "                 values=test_labels[:,0],\n",
        "                 start = offset,\n",
        "                 label=\"Test_data\")\n",
        "plot_time_series(timesteps=X_test[-len(test_windows):],\n",
        "                 values = tf.reduce_mean(model_3_preds, axis=1),\n",
        "                 fortmat=\"-\".\n",
        "                 start = offset,\n",
        "                 label=\"model_3_preds\")"
      ],
      "metadata": {
        "id": "x-VeCeQMvCkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which of our models is performing best so far?"
      ],
      "metadata": {
        "id": "CxOMSmRpvxhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({\"naive\":naive_results[\"mae\"],\n",
        "              \"horizon_1_window_7\":model_1_results[\"mae\"],\n",
        "              \"horizon_1_window_30\":model_2_results[\"mae\"],\n",
        "              \"horizon_7_window_30\" :model_3_results[\"mae\"]}, index=[\"mae\"]).plot(figsize=(10, 7), kind=\"bar\");"
      ],
      "metadata": {
        "id": "tfoAaPN0v4bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: Conv1D"
      ],
      "metadata": {
        "id": "5Cvm2ny4wbKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 1 # predict next day\n",
        "WINDOW_SIZE = 7 # Use previous week worth of data"
      ],
      "metadata": {
        "id": "oCI7tlMQwe69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create windowed dataset\n",
        "full_sindows, full_labels = make_windows(prices, window_size = WINDOW_SIZE, horizon=HORIZON)\n",
        "len(full_windows), len(full_labels)"
      ],
      "metadata": {
        "id": "PRQ86bCTwYZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train/test splits\n",
        "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\n",
        "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
      ],
      "metadata": {
        "id": "nDTbytfewU0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data sample shapes\n",
        "train_windows[0].shape # return (WINDOW_SIZE,)"
      ],
      "metadata": {
        "id": "r_Q7eeXQyfmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before we pass out data to the Conv1D layer, we have to reshape it in order to make usre it works\n",
        "x = tf.constant(train_windows[0])\n",
        "expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1)) # add anextra dimension for timesteps\n",
        "print(f\"Ofiginal shape : {x.shape}\") #( WINDOW_SIZE)\n",
        "print(f\"Expanded shape:{expand_dims_layer(x).shape}\") #  (WINDOW_SIZE, input_dim)\n",
        "print(f\"Original values with expanded shape:\\n {expand_dims_layer(x)}\")"
      ],
      "metadata": {
        "id": "fn8hWRUWyrxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model\n",
        "model_4 = tf.keras.Sequential([\n",
        "    # create Lambda layer to reshape inputs, without this layer, the model will error\n",
        "    layers.Lambea(lambda x:tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements\n",
        "    layers.Conv1D(filters=128, kernel_size = 5, padding=\"causal\", activation=\"relu\"),\n",
        "    layers.Dense(HORIZON)\n",
        "], name = \"model_4_conv1D\")\n",
        "\n",
        "# Compile model\n",
        "model_4.compile(loss=\"mae\",\n",
        "                optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "# Fit model\n",
        "model_4.fit(train_windows,\n",
        "            train_labels,\n",
        "            batch_size = 128,\n",
        "            epochs=100,\n",
        "            verbose=0,\n",
        "            validation_data = (test_windows, test_labels),\n",
        "            callbacks = [create_model_checkpoint(model_name=model_4.name)])"
      ],
      "metadata": {
        "id": "A5v148mxzv_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.summary()"
      ],
      "metadata": {
        "id": "GG9YWveT1sXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in best performing Conv1D model and evaluate it on the test data\n",
        "model_4 = tf.keras.models.load_model(\"model_expericment/model_4_conv1D\")\n",
        "model_4.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "zNoTHu7Z10QE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "model_4_preds = make_preds(model_4, test_windows)\n",
        "model_4_preds[:10]"
      ],
      "metadata": {
        "id": "Q7qOqhyU2E9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions\n",
        "model_4_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
        "                                 y_pred=model_4_preds)\n",
        "model_4_results"
      ],
      "metadata": {
        "id": "Ec0VRLl82Lyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5 : RNN (LSTM)"
      ],
      "metadata": {
        "id": "XvYbJMvX2gXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Let's build and LSTM model with the Functional API\n",
        "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
        "x = layers.Lambda(lambda x: tf.expand_dims(x, axis = 1))(inputs) # expand inputs dimension to be compatible with LSTM\n",
        "# print(x.shape)\n",
        "# x = layers.LSTM(128, activattion=\"relu\", reutrn,sequences = True)(x) # this layer will error if the inputs are not the right shape\n",
        "x = layers.SLOM(128, activaion=\"relu\")(x) # using the tanh loss function resi;ts om a massize error\n",
        "# print(x.shape)\n",
        "# Add another optional dense layer (you could add more of these to see if they improve model performance)\n",
        "# x = layers.Dense(32, activation=\"relu\")(x)\n",
        "output = layers.Dense(HORIZON)(x)\n",
        "model_5 = tf.keras.Model(inputs = inputs, outputs = output, name=\"model_5_lstm\")\n",
        "\n",
        "# Compile model\n",
        "model_5.compile(loss=\"mae\",\n",
        "                optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "# Sees when saving the model several warnings are appearing:\n",
        "model_5.fit(train_windows,\n",
        "            train_labels,\n",
        "            epochs =100,\n",
        "            verbose=0,\n",
        "            batch_size=128,\n",
        "            validation_data = (test_windows, test_labels),\n",
        "            callbacks=[create_model_checkpoint(model_name = model_5.name)])"
      ],
      "metadata": {
        "id": "Do_w1lg32kPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in best version of model 5 and evaluate on the test data\n",
        "model_5 = tf.keras.modesl.load_model(\"model_experiments/model_5_lstm/\")\n",
        "model_5.evaluate(test_windows, test_labels)"
      ],
      "metadata": {
        "id": "HIVTR2LU4eY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make apredictions with our LSTM model\n",
        "model_5_preds = make_preds(model_5, test_windows)\n",
        "model_5_preds[:10]"
      ],
      "metadata": {
        "id": "WJaU-3rl4sgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model 5 preds\n",
        "model_5_results = evaluate_preds(y_true = tf.squeeze(test_labels),\n",
        "                                 y_pred=model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "id": "eUKCRTYM4zDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a multivaiate time series"
      ],
      "metadata": {
        "id": "Dv774jrW5AKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make a multivariate time series\n",
        "bitcoin_prices.head()"
      ],
      "metadata": {
        "id": "7n-kKgbC5EeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block reward values\n",
        "block_reward_1 = 50 # 3 January 2009 (2009-01-03)\n",
        "block_reward_2 = 25 # 28 November 2012\n",
        "block_reward_3 = 12.5 # 9 July 2016\n",
        "block_reward_4 = 6.25 # 11 May 2020\n",
        "\n",
        "# Block reward dates (datetime form fo the above date stamps)\n",
        "block_reward_2_datetime = np.datetime64(\"2012-11-28\")\n",
        "block_reward_3_datetime = np.datetime64(\"2016-07-09\")\n",
        "block_reward_4_datetime = np.datetime64(\"2020-05-11\")"
      ],
      "metadata": {
        "id": "GNkv2ukx5J_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get date indexes for when to add in different block date\n",
        "block_reward_2_days = (block_reward_3_datetime - bitcoin_prices.index[0]).days\n",
        "block_reward_3_days = (block_reward_4_datetime - bitcoin_prices.index[0]).days\n",
        "block_reward_2_days, block_reward_3_days"
      ],
      "metadata": {
        "id": "5nzm6mBR5yj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add block_reward column\n",
        "bitcoin_prices_block = bitcoin_prices.copy()\n",
        "bitcoin_prices_block[\"block_reward\"] = None\n",
        "\n",
        "# Set values of block_reward column\n",
        "bitcoin_prices_block.iloc[:block_reward_2_days, -1] = block_reward_2\n",
        "bitcoin_prices_block.iloc[block_reward_2_days:block_reward_3_days, -1] = block_reward_3\n",
        "bitcoin_prices_block.iloc[block_reward_3_days:, -1] = block_reward_4\n",
        "bitcoin_prices_block.head()"
      ],
      "metadata": {
        "id": "B-J7a3XA6StA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the block reward/price over time\n",
        "# Note : Becuase of the different scales of our vlues we'll scale them to be between 0 and 1.\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "scaled_price_block_df = pd.DataFrame(minmax_scale(bitcoin_prices_block[[\"Price\",\"block_reward\"]]),\n",
        "                                     columns = bitcoin_prices_block.column,\n",
        "                                     index = bitcoin_prices_block.index)\n",
        "scaled_price_block_df.plot(figsize=(10, 7));"
      ],
      "metadata": {
        "id": "pQxSIqj47Gpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a windowed dataset with pandas"
      ],
      "metadata": {
        "id": "9vW6vk7x7w_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup dataset hapyerparameters\n",
        "HORIZON = 1\n",
        "WINDOW_SIZE = 7"
      ],
      "metadata": {
        "id": "AWfPSr8K73yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of the Bitcoin historical data with block reward feature\n",
        "bitcoin_prices_windowed = bitcoin_prices_block.copy()\n",
        "\n",
        "# Add windowed columns\n",
        "for i in range(WINDOW_SIZE) : # Shift values for each step in WINDOW_SIZE\n",
        "    bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(pericods=i+1)\n",
        "bitcoin_prices_windowed.head(10)"
      ],
      "metadata": {
        "id": "rUMWvojV79cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create X & y, remove the NaN's and convert to float32 to prevent TensorFlow errors\n",
        "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis = 1).astype(np.float32)\n",
        "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "LcjRtWbu8j9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View labels\n",
        "y.head()"
      ],
      "metadata": {
        "id": "z5sldolR9BR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make train and test sets\n",
        "split_size = int(len(x) * 0.8)\n",
        "X_train, y_train = X[:split_size], y[:split_size]\n",
        "X_test, y_test = X[split_size:],  y[split_size:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "id": "YZzT_pH5-RxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6: Dense (multivatiate times series)"
      ],
      "metadata": {
        "id": "XoY-ZC2z-mcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Make multivaiate tiem series model\n",
        "mdel_6 = tf.keras.Sequentail([\n",
        "    layers.Dense(128, activation=\"relu\"), # adding an extra layer here should lead to beating the naive model\n",
        "   # layers.Dense(128, activation=\"relu\"), # adding an extra layer here should lead to beating the naive model/\n",
        "    layers.Dense(HORIZON)\n",
        "], name=\"model_6_dense_multivatiate\")\n",
        "\n",
        "# Compile\n",
        "model_6.compile(loss=\"mae\",\n",
        "                optimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "# Fit\n",
        "model_6.fit(X_train, y_train,\n",
        "            epochs = 100,\n",
        "            batch_size = 128,\n",
        "            verbose = 0, # only print 1 line per epoch\n",
        "            callbacks=[create_model_checkpoint(model_name=model_6.name)])"
      ],
      "metadata": {
        "id": "0AGzqTm3-sKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OaxYjYXz_4Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure best model is loaded and evaluate\n",
        "model_6 = tf.keras.models.load_model(\"model_experiments/model_6_dense_multivariate\")\n",
        "model_6.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "Wx4HIayF_faU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on multivatiate data\n",
        "model_6_preds = tf.squeeze(model_6.predict(X_Test))\n",
        "model_6_preds[:10]"
      ],
      "metadata": {
        "id": "lB3cpbax_usj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate preds\n",
        "model_6_results = evaluate_preds(y_true = y_test,\n",
        "                                 y_pred = model_6_preds)\n",
        "model_6_results"
      ],
      "metadata": {
        "id": "ThzlrFO-_7Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "id": "HksLwxTUAGOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 7: N-BEATS algorithm"
      ],
      "metadata": {
        "id": "B21OzSnrAHbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building and testing an N-NEATS block layer"
      ],
      "metadata": {
        "id": "5Dva0bOuAMsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create NBeatsBlock custom layer\n",
        "class NBeatsBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, # the constructor takes all the hyperparameters for the layer\n",
        "                 input_size:int,\n",
        "                 theta_size:int,\n",
        "                 horizon:int,\n",
        "                 n_neurons:int\n",
        "                 n_layers:int,\n",
        "                 **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainiable, name)\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_size = input_size\n",
        "        self.theta_size = theta_size\n",
        "        self.horizon = horizon\n",
        "        self.n_neurons = n_neurons\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # Block contains stack of $ fully connected layers each has ReLU activation\n",
        "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n",
        "        # Output of block is a theta layer with linear activation\n",
        "        self.theta_layer = tf.keras.layers.Dense(theta_size, activation = \"linear\", name=\"theta\")\n",
        "\n",
        "    def call(self, inputs) : # the call mehtos is what runs when the layer is clled\n",
        "        x = inputs\n",
        "        for layer in self.hidden:\n",
        "            x = layer(x)\n",
        "        theta = self.theta_layer(x)\n",
        "        # Output the backcast and forecast from theta\n",
        "        backcast, forecast =theta[:, :self.input_size], theta[:, -self.horizon:]\n",
        "        return backcas, forecast"
      ],
      "metadata": {
        "id": "mG6qlIpUASgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up dummy NBeatsBlock layer to represent inputs and outputs\n",
        "dummy_nbeats_block_layer = NBeatsBlock(inputs_size=WINDOW_SIZE,\n",
        "                                       theta_size=WINDOW_SIZE + HORIZON, #  backcast + forecast\n",
        "                                       horizon = HORIZON,\n",
        "                                       n_neurons = 128,\n",
        "                                       n_layers = 4)"
      ],
      "metadata": {
        "id": "RZszXDOSDIb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy inputs (have to be same size as input_size)\n",
        "dummy_inputs = tf.expand_dims(tf.range(WINDOW_SIZE)+1, axis=0) # input shape to the model has to reflect Dense layer input requirements (ndim=2)\n",
        "dummy_inputs"
      ],
      "metadata": {
        "id": "BBdzsAIoDhXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass dummy inputs to dummy NBeatsBlock layer\n",
        "backcast.forecast  = dummy_nbeats_block_layer(dummy_inputs)\n",
        "# These are the activation outputs of the theta layer(they'll be random due to no training of the model)\n",
        "print(f\"Backcast:{tf.squeeze(backcast.numpy())}\")\n",
        "print(f\"Forecast:{tf.squeeze(forecast.numpy())}\")"
      ],
      "metadata": {
        "id": "0-bkViExD2Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing data for the N-BEATS algotithm using `tf.data`"
      ],
      "metadata": {
        "id": "DCMh_H9cEWWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HORIZON = 1 # how far to predict forward\n",
        "WINDOW_SIZE = 7 # how far to lookback"
      ],
      "metadata": {
        "id": "UWelzTZgEePM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create NBEATS data inputs ( NBEATS works with univariate time series)\n",
        "bitcoin_prices.head()"
      ],
      "metadata": {
        "id": "27e-gZ5kEoxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add windowed column\n",
        "bitcoin_prices_nbeats = bitcoin_prices.copy()\n",
        "for i in range(WINDOW_SIZE):\n",
        "    bitcoin_prices_nbeats[f\"Price+{i+1}\"] = bitcoin_prices_nbeats[\"Price\"].shift(periods=i+1)\n",
        "bitcoin_prices_nbeats.dropna().head()"
      ],
      "metadata": {
        "id": "SjPMLMJDExcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make features and labels\n",
        "X = bitcoin_prices_nbeats.dropna().drop(\"Price\", axis=1)\n",
        "y = bitcoin_prices_nbeats.dropna()[\"Price\"]\n",
        "\n",
        "# Make train and test sets\n",
        "splti_size = int(len(X)*0.8)\n",
        "X_train, y_train = X[:split_size], y[:split_size]\n",
        "X_test, y_test = X[split_size:], y[split_size:]\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "id": "-Qb4Za_XFNAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Trun train and test arrays into tensor Datasets\n",
        "train_feature_dataset = tf.data.dataset.from_tensor_slices(X_train)\n",
        "train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "\n",
        "test_features_datset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "\n",
        "# 2. Combine Features & labels\n",
        "train_dataset = tf.data.Dataset.Zip((train_features_dataset, train_labels_dataset))\n",
        "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_datset))\n",
        "\n",
        "# 3. Bath and prefetch for optimal performance\n",
        "BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(Tf.data.AUTOTUNE)\n",
        "\n",
        "train_datset, test_dataset"
      ],
      "metadata": {
        "id": "gtLPAnH_F3mT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up hyperparameters for N-BEATS algorithm"
      ],
      "metadata": {
        "id": "0PPeoE5GK4vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vlaues from N-BEATS paper Figure 1 and Table 18/Appendix D\n",
        "N_EPOCHS = 5000 # called \"Iterations\" it table 18\n",
        "N_NEURONS = 512 # called \"Width\" in Table 18\n",
        "N_LAYERS = 4\n",
        "N_STACKS = 30\n",
        "\n",
        "INPUT_SIZE = WINDOW_SIZE * HORIZON # called \"Lookback\" in Table 18\n",
        "THETA_SIZE = INPUT_SIZE + HORIZON\n",
        "\n",
        "INPUT_SIZE, THETA_SIZE"
      ],
      "metadata": {
        "id": "JKsk7m1tK_22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting ready for residual connections"
      ],
      "metadata": {
        "id": "rJhXW_yJLet1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make tensors\n",
        "tensor_1 = tf.range(10) + 10\n",
        "tensor_2 = tf.range(10)\n",
        "\n",
        "# Subtract\n",
        "subtracted = layers.subtract([tensor_1, tensor_2])\n",
        "\n",
        "# Add\n",
        "added = layers.add([tensor_1, tensor_2])\n",
        "\n",
        "print(f\"Input tensors : {tensor_1.numpy()} & {tensor_2.numpy()}\")\n",
        "print(f\"Subtracted : {subtracted.numpy()}\")\n",
        "print(f\"Added : {added.numpy()}\")"
      ],
      "metadata": {
        "id": "4CUdWCFeLkAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Builing, compiling and fitting the N-BEATS algorithm"
      ],
      "metadata": {
        "id": "fatp9XygMDtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# 1. Setup N-BEATS Block layer\n",
        "nbeats_block_layer = NBeatsBlock(input_size = INPUT_SIZE,\n",
        "                                 theta_size = THETA_SIZE,\n",
        "                                 horizon = HORIZON,\n",
        "                                 n_n3eurons = N_NEURONS,\n",
        "                                 n_layers = N_LAYERS,\n",
        "                                 name=\"InitialBlock\")\n",
        "\n",
        "# 2. Create input to stacks\n",
        "stack_input = layers.Input(shape=(INPUT_SIZE), name = \"stack_input\")\n",
        "\n",
        "# 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)\n",
        "backcast.forecast = nbeats_block_layer(stack_input)\n",
        "residuals = layers.substract([stack_input, backcast], name = f\"subtract_00\")\n",
        "\n",
        "# 4.create stacks of blocks\n",
        "for i, _ in enumerate(range(N_STACKS-1)) : # first stack is already creted in (3)\n",
        "\n",
        "    # 5. Use the NBeatsBlock to calcuate the backcast as well as block forecast\n",
        "    backcast, block_forecast = NBeatsBlock(\n",
        "        input_size = INPUT_SIZE,\n",
        "        theta_size = THETA_SIZE<\n",
        "        horizon=HORIZON,\n",
        "        n_neurons = N_NEURONS,\n",
        "        n_layers=N_LAYERS,\n",
        "        name=f\"NBeatsBlock_{i}\"\n",
        "    )(residuals) # pass it in residuals (the backcast)\n",
        "\n",
        "    # 6. Create the double residual stacking\n",
        "    residuals = layers.subtract([residual, backcast], name=f\"substract_{i}\")\n",
        "    forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n",
        "\n",
        "# 7. put the stack model together\n",
        "model_7 tf.keras.Model(inputs = stack_inptut,\n",
        "                       outpus = forcast, name=\"model_7_N-BEATS\")\n",
        "\n",
        "# 8. Compile with MAE loss and Adam optimizer\n",
        "model_7.compile(loss=\"mae\",\n",
        "                optimizer = tf.keras.optimizers.Adam(0.001),\n",
        "                metircs=[\"mae\",\"mse\"])\n",
        "\n",
        "# 9. Fit the model with EarlyStopping and ReducelROnPlateau callbacks\n",
        "model_7.fit(train_dataset,\n",
        "            epochs =N_EPOCHS,\n",
        "            validation-data = test_datset,\n",
        "            verbose = 0, # prevent large amounts of training outputs\n",
        "            # callbacks = [create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=200, restroe_best_weights = True),\n",
        "                       tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)])"
      ],
      "metadata": {
        "id": "5yVqMvhIMKaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalutate N-BEATS model on the test dataset\n",
        "model_7.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "X_MGKgpSP5Fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with N-BEATS model\n",
        "model_7_preds = make_preds(model_7, test_dataset)\n",
        "model_7_preds[:10]"
      ],
      "metadata": {
        "id": "G9l_puUIP_6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate N-BEATS model predictions\n",
        "model_7_results = evaluate_preds(y_true = y_test,\n",
        "                                 y_pred = model_7_preds)\n",
        "model_7_results"
      ],
      "metadata": {
        "id": "tCFn1D8hQIcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the N-BEATS architecture we've create"
      ],
      "metadata": {
        "id": "BnRZmUQiQTiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the N-BEATS model and inspect the architecture\n",
        "from tensorflow.keras.uitls import plot_model\n",
        "plot_model(model_7)"
      ],
      "metadata": {
        "id": "caNzuNRHQXa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will error out unless a \"get_config()\" method is implemented -  this could be extra_curriculum\n",
        "model_7.save(model_7.name)"
      ],
      "metadata": {
        "id": "wVmNVWpEO9qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 8: Create an ensemble (stacking different models together)"
      ],
      "metadata": {
        "id": "EkMBTmGcRHSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constructing and fitting and ensembel of models (using different loss functions)"
      ],
      "metadata": {
        "id": "crw1-Up_RMzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ensembel_models(horizon=HORIZON,\n",
        "                        train_data = train_dataset,\n",
        "                        test_data = test_dataset,\n",
        "                        num_iter = 10,\n",
        "                        num_epochs = 100,\n",
        "                        loss_fns =[\"mae\",\"mse\",\"mape\"]):\n",
        "    # Make empty list for trained ensemble models\n",
        "    ensemble_models = []\n",
        "\n",
        "    # create num_iter number of models per loss function\n",
        "    for i in rnage(num_iter):\n",
        "        # Build and fit a new model with a different loss function\n",
        "        for loss_funtion in loss_fns:\n",
        "            print(f\"Opitmizing model by reducing : {loss_function} for {num_epochs} epochs, model number : {i}\")\n",
        "            model = tf.keras.Sequential([\n",
        "                # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction\n",
        "                layers.Dense(128, kernel_initializer = \"he_normal\", activation=\"relu\"),\n",
        "                layers.Dense(128, kernel_initializer = \"he_normal\", activation = \"relu\"),\n",
        "                layers.Dense(HORIZON)\n",
        "            ])\n",
        "\n",
        "            # Compile simple model with current loss function\n",
        "            model.cimpile(loss=loss_function,\n",
        "                          optimizer = tf.keras.optimzers.Adam(),\n",
        "                          metrics=[\"mae\",\"mse\"])\n",
        "\n",
        "            # fit model\n",
        "            model.fit(train_data,\n",
        "                      epochs = num_epochs,\n",
        "                      verbose=0,\n",
        "                      validation_data = test_data,\n",
        "                      # Add callbacks to prevent training from goingstalling for too long\n",
        "                      callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\".\n",
        "                                                                  patience=200,\n",
        "                                                                  restor_best_weights=True),\n",
        "                                 tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                                      patience=100,v\n",
        "                                                                      verbose=1)])\n",
        "\n",
        "            # Append fitted model to list of ensembel models\n",
        "            ensemble_mdoels.append(model)\n",
        "\n",
        "    return ensemble_models # return list of trained models"
      ],
      "metadata": {
        "id": "JOShuZrKLjOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Get list of trained ensemble models\n",
        "ensemble_model = get_ensemble_models(num_iter = 5,\n",
        "                                     num_epochs = 1000)"
      ],
      "metadata": {
        "id": "JxkOIdwrTmLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making predictions with an ensemble model"
      ],
      "metadata": {
        "id": "w1k5_eA7TyAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function which uses a list of trained models to make and return a list of predictions\n",
        "de make_ensemble_preds(ensemble_models, data):\n",
        "ensemble_preds=[]\n",
        "for model in ensemble_models:\n",
        "    preds = model.predict(data) # make predictions with current ensembel model\n",
        "    ensemble_preds.append(preds)\n",
        "return tf.constant(tf.squeeze(ensembel_preds))"
      ],
      "metadata": {
        "id": "F5XFsfIAT4gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of ensemble predictions\n",
        "ensemble_preds = make_ensemble_preds(ensemble_models = ensemble_models,\n",
        "                                     data=test_dataset)\n",
        "ensemble_preds"
      ],
      "metadata": {
        "id": "-AZfm3uzUUCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate ensemble model(s) predictions\n",
        "ensemble_results = evalualte(y_true = y_test,\n",
        "                             y_pred = np.median(ensemble_preds, axis = 0)) # take the median across all ensemble predictions\n",
        "ensemble_results"
      ],
      "metadata": {
        "id": "QVOIzZEPUfAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the prediction intervals(uncertainity estimates) of our ensemble"
      ],
      "metadata": {
        "id": "eDH5GMHQU2et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find upper and lower bounds of ensemble predictions\n",
        "def get_uppder_lower(preds): # 1. Take the predictions of multiple randomly initializer deep learning neural networks\n",
        "\n",
        "    # 2. Measure the standard deviation of the predictions\n",
        "    std = tf.math.reduce_std(preds, axis = 0)\n",
        "\n",
        "    # 3. Multiply the standard devation by 1.96\n",
        "    interval = 1.96 * std\n",
        "\n",
        "    # 4. Get the prediction interval upper and lower bounds\n",
        "    preds_mean = tf.reduce_mean(preds, axis = 0)\n",
        "    lower,upper = preds_mean - inverval, preds_mean + interval\n",
        "    return lower, upper\n",
        "\n",
        "# Get the upper and lower bounds of the 95%\n",
        "lower, upeer = get_upper_lowe(preds=ensembel_preds)"
      ],
      "metadata": {
        "id": "Nl4hKZ-qU9LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the median values of our ensemble preds\n",
        "ensemble_median = np.median(emsemble_preds, axis = 0)\n",
        "\n",
        "# Plot the median of our ensemble pred along with the prediciton intervals( where the predictions fall between)\n",
        "offset = 500\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(X_test.index[offset:], y_test[offset:], \"g\", label=\"Test Data\")\n",
        "plt.plot(X_test.index[offset:], ensemble_median[offset:], \"k-\", label=\"Ensemble Median\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"BTC Price\")\n",
        "plt.fill_between(X_test.index[offset:],\n",
        "                 (lower)[offset:],\n",
        "                 (upper)[]offset:, label=\"Prdiction Intervals\")\n",
        "plt.legend(loc=\"upper left\", fontsize = 14);"
      ],
      "metadata": {
        "id": "rjrSgPV_V5ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aside: two types of uncertainity ( coconut and subway)"
      ],
      "metadata": {
        "id": "Rg34hYiEa1p9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uncerainity in dating"
      ],
      "metadata": {
        "id": "7aTQDSDta9zM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 9 : Train a model on the full hisotrical data to maek predictions into future"
      ],
      "metadata": {
        "id": "VPbTrh8ZbEsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bitcoin_prices_windowed.head()"
      ],
      "metadata": {
        "id": "WG-nXoDMbNn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model on entire data to make predicion for the next day\n",
        "X_all = bitcoin_prices_windowed.drop([\"Price\",\"block_reward\"], axis = 1).dropna().to_numpy() # only want prices, our future model can ve a university model\n",
        "y_all = bitcoin_prices_windowed.ropna()[\"Price\"].to_numpy()"
      ],
      "metadata": {
        "id": "vjTpBRg1bUdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Turn X and y into tensor Datasets\n",
        "feaures_dataset_all = tf.data.Dataset.from_tensorl_slices(X_all)\n",
        "labels_dataset_all = tf.data.Dataset.from_teson_slices(y_all)\n",
        "\n",
        "# 2. Combine features & labels\n",
        "dataset_all = tf.data.Dataset.zip((features_dataset_all, labels_dataset_all))\n",
        "\n",
        "# 3. Batch and prefetch for optimal performance\n",
        "BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\n",
        "dataset_all = dataset_all.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "dataset_all"
      ],
      "metadata": {
        "id": "YdfRflxGcyQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create model (nice and simple, just to test)\n",
        "model_9 = tf.keras.Seauential([\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(HORIZON)\n",
        "])\n",
        "\n",
        "# Compile\n",
        "model_9.compile(loss=tf.keras.losses.mae,\n",
        "                optimizer = tf.keras.optimizers.Adma())\n",
        "\n",
        "# Fit model on all of the data to make future forecasts\n",
        "model_9.fit(dataset_all,\n",
        "            epochs = 100,\n",
        "            verbose = 0) # don't print out anything, we've ween this all before"
      ],
      "metadata": {
        "id": "MDcwfkPOdf3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predicitons on the future"
      ],
      "metadata": {
        "id": "VcQVebx3eFLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many timestepsm to predict into the future?\n",
        "INTO_FUTURE = 14 # since our Bitcoin data is daily, this is for 14 days"
      ],
      "metadata": {
        "id": "BMr_tDL4eIQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create function to make predictions into the future\n",
        "def make_furue_forecast(values, model, into_future, window_since = WINDOW_SIZE) -> list:\n",
        "\n",
        "    # 2. MaKE an empty list for future forecasts/prepare data to forecast on\n",
        "    future_forecast=[]\n",
        "    last_window = values[-WINDOW_SIZE:] # only want pred from the last window (this will get updated)\n",
        "\n",
        "    # 3.Make INTO_FURUE number of predictions, altering the dat which gets predicted on each time\n",
        "    for _ in range(into_future):\n",
        "\n",
        "        # Predict on last window then append it again, again, again (model start to make forecasts on its own fore casts)\n",
        "        future_pred = model.predict(tf.expand_dims(las_window, axis=0))\n",
        "        print(f\"Predicting on:\\m {last_window} -> Prediction : {tf.squeeze(furue_pred).numpy()}\\m\")\n",
        "\n",
        "        # Append prediction to future_forecast\n",
        "        future_forecast.append(tf.squeeze(future_pred).numpy())\n",
        "        # print(future_forecast)\n",
        "\n",
        "        # Update last window with new pred and get WINDOW_SIZE most recent preds (model war trained on WINDEO_SIZE windows)\n",
        "        last_window = np.append(last_window, future_pred)[-WINDOW_SIZE:]\n",
        "\n",
        "    return future_forecast"
      ],
      "metadata": {
        "id": "anKwNsyoeUNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make forcasts into future of the price ot Bitcoin\n",
        "# Note : if you're reading this at a later date, you may already be in the future, so the forecasts\n",
        "# we're making may not actually be forecasts, if that's the case,readjust the training data.\n",
        "future_forecast = make_future_forecast(values=y_all,\n",
        "                                       model=model_9,\n",
        "                                       into_furue = INTO_FUTURE,\n",
        "                                       window_size = WINDOW_SIZE)"
      ],
      "metadata": {
        "id": "kt12zY2if84x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "future_forecast[:10]"
      ],
      "metadata": {
        "id": "qumwNnjmgh3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot future forecasts"
      ],
      "metadata": {
        "id": "Nyeqr3xAglfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_furue_dates(start_date, into_future, offset = 1):\n",
        "    start_date = start_date + np.timedelta64(offset, \"D\") # specify start date, \" D\" stands for day\n",
        "    end_date = start-date + np.timedelta64(into_furue, \"D\") # specify end date\n",
        "    return np.arange(start_date, end_date, dtype=\"datetime64[D]\") # return a date range between start date and end date"
      ],
      "metadata": {
        "id": "5Yq2C5mVgosD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Last time setp of timestpes( currently in np.datetime64 format)\n",
        "last_timestep = bitcoin_prices.indes[-1]\n",
        "last_timestep"
      ],
      "metadata": {
        "id": "Ad_MeSh74hD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get next two weeks of timesteps\n",
        "next_time_steps = get_future_dates( start_date = last_timestep,\n",
        "                                   info_future = INTO_FUTURE)\n",
        "next_time_steps"
      ],
      "metadata": {
        "id": "yr70zGRS4xdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert last timestep/final price so the graph doesn't look messed\n",
        "next_time_steps = np.insert(next_time_steps, 0, last_timestep)\n",
        "future_forecast = np.insert(future_forecast, 0, btc_price[-1])\n",
        "next_time_steps, future_forecast"
      ],
      "metadata": {
        "id": "1S9C6IZi4-jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot future price predictions of Bitcoin\n",
        "plt.figure(figsize=(10,7))\n",
        "plot_time_series(bitcoin_prices.indes, btc_price, start = 2500, format=\"-\", label = \"Actual BTC Price\")\n",
        "plot_time_series(next_time_steps, furue_forecast, format =\"-\", label=\"Predicted BTR Price\")"
      ],
      "metadata": {
        "id": "aZxC8oW_5UwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 10: Why forecasting is BS (the turkey problem)"
      ],
      "metadata": {
        "id": "Ef8H2-cg5uOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's introduce a Turkey problem to our BTC data ( price BTC falls 100x in one day)\n",
        "btc_price_turkey = btc_price.copy()\n",
        "btc_price_turkey[-1] = btc_price_turkey[-1] /100"
      ],
      "metadata": {
        "id": "sgIGseZz54vR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manufacture an extra price on the end (to showcas the Turkey problem)\n",
        "btc_price_tukey[-10:]"
      ],
      "metadata": {
        "id": "6DVI8Fua6N9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the timesteps for the turkey problem\n",
        "btc_timesteps_turkey = np.array(bitcoin_prices.index)\n",
        "btc_timesteps_turkey[-10:]"
      ],
      "metadata": {
        "id": "uJ4P3oxn6X3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.fifure(figsize=(10, 7))\n",
        "plot_time_series(timesteps=btc_timesteps_turkey,\n",
        "                 values = btc_price_turkey,\n",
        "                 format = \"-\",\n",
        "                 label = \"BTC Price + Turkey Problem\",\n",
        "                 start = 2500)"
      ],
      "metadata": {
        "id": "mTlSY84l6isA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test set for turkey problem data\n",
        "full_windows, full_labels = make_windows(np.array(btc_price_turkey),\n",
        "                                         window_size = WINDOW_SIZE, horizon = HORIZON)\n",
        "X_train, X_test, y_train, y_test = make_train_test_splits(full_windows, full_labels)\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "0CwO-wC460ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a turkey model (model to predict on turkey data)"
      ],
      "metadata": {
        "id": "clzDYMWd7X37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone model 1 archetecture for turkey model and fit the turkey model on the turkey data\n",
        "turkey_model = tf.keras.models.clone_model(model_1)\n",
        "turkey_model._nam,e = \"Turkey_Model\"\n",
        "turkey_model.compile(loss=\"mae\", otimizer = tf.keras.optimizers.Adam())\n",
        "\n",
        "turkey_mode.fit(X_train, y_train,\n",
        "                epochs = 100,\n",
        "                verbose = 0,\n",
        "                validation_data = (X_test, y_test),\n",
        "                callbacks=[create_model_checkpoint(turkey_model.name)])"
      ],
      "metadata": {
        "id": "umoVAM1j7c1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate turkey model on test data\n",
        "turkey_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "mMUA2Bnz8CNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model and evaluate on test data\n",
        "turkey_model = tf.keras.models.load_model(\"model_experiments/Turkey_Model/\")\n",
        "turkey_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "8CtmCNe38IY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with Turkey model\n",
        "turkey_preds = make_preds(turkey_model, X_test)\n",
        "turkey_preds[:10]"
      ],
      "metadata": {
        "id": "z1zyBoU48Xwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate turkey preds\n",
        "turkey_results = evaluate_preds(y_true = y_test,\n",
        "                                y_pred = turke_preds)\n",
        "turkey_results"
      ],
      "metadata": {
        "id": "rnVnRUHg8jiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results"
      ],
      "metadata": {
        "id": "RkgcNswd8tVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "offset=300\n",
        "plot_time_series(timesteps = btc_timesteps_turkey[-len(X_test): ],\n",
        "                 vlaues = btc_price_turkey[-len(y_test):],\n",
        "                 format=\"-\",\n",
        "                 label=\"Turkey Test Data\", start = offset)\n",
        "plot_time_series(timesteps=btc_timesteps_turkey[-len(X_test):],\n",
        "                 values = turke_preds,\n",
        "                 label = \"Turkey Preds\",\n",
        "                 start = offset);"
      ],
      "metadata": {
        "id": "mNYN65IZ_qBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Models"
      ],
      "metadata": {
        "id": "YFDz7OQCASw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare different model results(w=window, h=horizon, e.g. w=7 means a window size of 7)\n",
        "model_results = pd.DataFrame({\"naive_model\": naive_results,\n",
        "                              \"model_1_dense_w7_h1\": model_1_results,\n",
        "                              \"model_2_dense_w30_h1\" :model_2_results,\n",
        "                              \"model_3_dense_w30_h7\": model_3_results,\n",
        "                              \"model_4_CONV1D\": model_4_results,\n",
        "                              \"model_5_LSTM\": model_5_results,\n",
        "                              \"model_6_multivariate\":model_6_results,\n",
        "                              \"model_8_NBEATS\":model_7_results,\n",
        "                              \"model_9_ensembel\":ensemble_results,\n",
        "                              \"model_10_turkey\":turkey_results}).T\n",
        "model_results.head(10)"
      ],
      "metadata": {
        "id": "bLSUKS35AVR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort model results by MAE and plot them\n",
        "model_results[[\"mae\"]].sort_values(by=\"mae\").plot(figsize=(10,7), kind=\"bar\");"
      ],
      "metadata": {
        "id": "vDWtsvscBWWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LMy-rOOmBiGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4n272Wmm4xEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4fZGlR6bCfn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}