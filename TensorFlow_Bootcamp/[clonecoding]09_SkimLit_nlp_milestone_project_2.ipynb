{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 09. Milestone Project 2 : SkimLit"
      ],
      "metadata": {
        "id": "3dMnjjLt1aX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import datetime\n",
        " print(f\"Notebook last run (end-to-end) : {datetime.datetime.now()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Get5Iz16W4",
        "outputId": "889cbf99-63db-415f-ba64-cfb9c0d68e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook last run (end-to-end) : 2025-06-04 12:13:22.065409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for GPU\n",
        "!nvidai-smi -L"
      ],
      "metadata": {
        "id": "0DStq28T3CLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data\n",
        "\n",
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ],
      "metadata": {
        "id": "8cBrGa403HhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what files are in the PubMed_20K dataset\n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
      ],
      "metadata": {
        "id": "K6njofsc3XD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by using the 20k dataset\n",
        "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ],
      "metadata": {
        "id": "xfDVwQVl39El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all of the file names in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "id": "o4jaCEOd4DiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "Jk6gOdy34NAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        return f.readlines()"
      ],
      "metadata": {
        "id": "RZZgSNwb4P_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = get_lines(data_dir + \"trai.txt\")\n",
        "train_lines[:20]"
      ],
      "metadata": {
        "id": "92ymvKT84il3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_with_line_numbers(filename):\n",
        "    input_lines = get_lines(filename) # get all lines form filename\n",
        "    abstract_lines = \"\" # create an empty abstract\n",
        "    abstract_samples = [] # create an empty list og abstract\n",
        "\n",
        "    # loop through each line in target file\n",
        "    for line in inputs_lines:\n",
        "        if line.stratswith('###'): # check to see if line is an ID line\n",
        "            abstract_id = line\n",
        "            abstract_liens = \"\" # reset abstract string\n",
        "        elif line.issapce(): # check to see if line is a new line\n",
        "            abstract_line_split = abstract_lines.splitlines() # split abstract into seaprate liens\n",
        "\n",
        "            # Iterate through eacth line in absract and count them at the same time\n",
        "            for abstract_line_number, abstract_lin in enumerate(abstract_line_split):\n",
        "                line_data = {} # create empty dict to stroe data from line\n",
        "                target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "                line_data[\"target\"] = target_text_split[0] # get target label\n",
        "                line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "                line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "                line_data[\"totalt_lines\"] = len(abstract_line_split) - 1 # how may total lines are in the abstract? (start from 0)\n",
        "                abstract_samples.appen(line_data) # add line data to abstract samples list\n",
        "\n",
        "        else: # if the above conditions aren't fulfilled, the line contrains labelled sentecne\n",
        "            abtsract_lines += line\n",
        "\n",
        "    return abstract_samples\n",
        "\n"
      ],
      "metadata": {
        "id": "p7tfSzsW4tFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bet data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + 'test.txt')\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "metadata": {
        "id": "rF_d2ZJ571MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the fisrt abstract of our training data\n",
        "train_samples[:14]"
      ],
      "metadata": {
        "id": "PWwxsH308eOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFram(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "PciLMnDf8kyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of labels in training data\n",
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "id": "ei5N1DPW9oPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.total_lines.plot.hist()"
      ],
      "metadata": {
        "id": "ePff7b1A9xAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get list of sentences"
      ],
      "metadata": {
        "id": "vwQREpK690Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert abstract text lines into list\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df['text'].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "metadata": {
        "id": "UnvQbfZl-AZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View first 10 lines of training senteces\n",
        "train_sentences[:10]"
      ],
      "metadata": {
        "id": "P64XhNtU-oe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make numeric labels (ML models require numeric labels)\n"
      ],
      "metadata": {
        "id": "inJZPfy0-vjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encode labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_one_hot"
      ],
      "metadata": {
        "id": "inV58s4U-40G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label encode labels"
      ],
      "metadata": {
        "id": "mzY6Z2P__rUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels (\"target\" columns) and encode them into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test-df[\"target\"].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "clfpKv3L_uWk",
        "outputId": "bd39a8a8-216c-4d66-d302-6e442110e018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-558b7d0fb00a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_labels_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_labels_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class nsames and number of class from labelEncoder instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ],
      "metadata": {
        "id": "ljMPk92yAUof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a series of model experiments"
      ],
      "metadata": {
        "id": "rq2iYnrxAg5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Getting a baseline"
      ],
      "metadata": {
        "id": "b8oqiqOEAki2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "model_0 = Pipeline([\n",
        "    (\"tf-idf\", TfidVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(X=train_sentences,\n",
        "            y=train_labels_encoded);"
      ],
      "metadata": {
        "id": "cRSvoR-mAt9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalutate baseline on validation dataset\n",
        "model_0.score(X=val_sentences,\n",
        "              y=val_labels_encoded)"
      ],
      "metadata": {
        "id": "3fMAtU9VAJq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds"
      ],
      "metadata": {
        "id": "rWKh2yc-Bltr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download helper function script\n"
      ],
      "metadata": {
        "id": "BQgEcdqEBsT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download helper functions script\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "id": "OGPlZePyByfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import calculate_results helper function\n",
        "from helper_functions import calculate_results"
      ],
      "metadata": {
        "id": "RalGTb15B2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate baseline results\n",
        "baseline_results = calculate_results(y_true = val_labels_encoded,\n",
        "                                     y_pred = baseline_preds)\n",
        "\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "Gfxi5btLB-fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing our data for deep sequence models"
      ],
      "metadata": {
        "id": "fHGBjJoMCMIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "dT40t8MGCSmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long is each sentence on average?\n",
        "sent_lens = [len(sentence.split()) for sentence in train_senteces]\n",
        "avg_sent_len = np.mean(sent_lens)\n",
        "avg_sent_len # return average setence length (in tokens)"
      ],
      "metadata": {
        "id": "uwKYCPE0CY6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the distribution look like?\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_lens, bins = 7)"
      ],
      "metadata": {
        "id": "jxRcj30lCrv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How Long ot a sentence covers 95% of the lengths?\n",
        "output_seq_len = int(np.percentiles(sent_lens, 95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "id": "myP9GYKUCzbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum sentence lenth in the training set\n",
        "max(sent_lens)"
      ],
      "metadata": {
        "id": "4EiQWG1wC-ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create text vectorizer"
      ],
      "metadata": {
        "id": "IvL86dPpVZdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words are in our vocabulary?\n",
        "max_tokens = 68000"
      ],
      "metadata": {
        "id": "IL_fr0JzCQOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text vectorizer\n",
        "\n",
        "# After TensorFlow 2.6\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokesn, #number of word in vocabulary\n",
        "                                    output_sequence_length=55) # desired output length of vectorized sequences"
      ],
      "metadata": {
        "id": "AUquPRxOVqiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt text vectorizer to training sentences\n",
        "text_vectorizer.adat(train_setences)"
      ],
      "metadata": {
        "id": "7TkEwT8-WEOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out text vectorizer\n",
        "import random\n",
        "target-sentece = random.choice(train_sentences)\n",
        "print(f\"Text : \\n{target_sentence}\")\n",
        "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
        "print(f\"\\nVectorized text:\\n {text_vectorizer([target_sentence])}\")"
      ],
      "metadata": {
        "id": "9VX0cnmdWK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words in our training vacabulary?\n",
        "rct_20k_text_voca = text_vectorizer.get_vocabulary()\n",
        "print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\")\n",
        "print(f\"Most common words in the vocabulary : {rct_20k_text_vocab[:5]}\")\n",
        "print(f\"Least common words in the vocabulary : {rct_20k_text_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "vEVEa8M3WhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the config of our text vectorizer\n",
        "text_vectorizer.get_config()"
      ],
      "metadata": {
        "id": "xniyO2R_ZRsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create custom text embedding"
      ],
      "metadata": {
        "id": "txsFT1okZW-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create toekn embedding layer\n",
        "tocken_embed = laeyrs.Embedding(input_dim = len(rct_20k_text_vocab), # length of vocabulary\n",
        "                                output_dim=128, # Note : different embeding size result in drasticaaly different numbers of parameters to train\n",
        "                                # Use masking to handle variable sequence lengths (save space)\n",
        "                                mask_zero = True,\n",
        "                                name=\"token_embedding\")\n",
        "# Show example embedding\n",
        "print(f\"Sentence before vectorization : \\n {target_sentence}\\n\")\n",
        "vectorized_sentence = text_vectorizer([target_sentence])\n",
        "print(f\"Sentence after vectorization (before embedding) : \\n{vectorized_sentence}\\n\")\n",
        "embedded_sentence = token_embed(vectorzed_sentence)\n",
        "print(f\"Sentence after embedding : \\n {embedded_sentence}\\n\")\n",
        "print(f\"Embedded sentence shape : {embedded_sentence.shape}\")"
      ],
      "metadata": {
        "id": "f2AE8SgmZZzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datasets (as fast as possible)"
      ],
      "metadata": {
        "id": "pjtwc77jbLg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into tensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slice((train_senteces, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slice((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slice((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "UJEM7XRMbQYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the TensorSliceDataset's and turn them into prefectched batches\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "xE2doIW1b1Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 : Conv1D with token embeddings"
      ],
      "metadata": {
        "id": "N9TaA-jgcNi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 1D convolutional model to process sequences\n",
        "inputs = layers.Input(shape = (1,), dtype = tf.string)\n",
        "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
        "token_embeddings = token_embed(text_vectors) # create embedding\n",
        "x = layers.Conv1D(64, kernel_size = 5, padding = \"same\", activation = \"relu\")(token_embeddings)\n",
        "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
        "outputs = layers.Dense(num_classes, activation = \"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile\n",
        "model_1.compile(loss = \"categorical_crossentorpy\", # if your labels are integer form (ont one hot) use sparse_categorical_crossentropy\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = [\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "xqVfjYCTcSsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HVNJGMPMdeYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}