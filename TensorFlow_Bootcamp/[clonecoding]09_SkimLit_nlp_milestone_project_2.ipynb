{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 09. Milestone Project 2 : SkimLit"
      ],
      "metadata": {
        "id": "3dMnjjLt1aX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import datetime\n",
        " print(f\"Notebook last run (end-to-end) : {datetime.datetime.now()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Get5Iz16W4",
        "outputId": "889cbf99-63db-415f-ba64-cfb9c0d68e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Notebook last run (end-to-end) : 2025-06-04 12:13:22.065409\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check for GPU\n",
        "!nvidai-smi -L"
      ],
      "metadata": {
        "id": "0DStq28T3CLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data\n",
        "\n",
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ],
      "metadata": {
        "id": "8cBrGa403HhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what files are in the PubMed_20K dataset\n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
      ],
      "metadata": {
        "id": "K6njofsc3XD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start by using the 20k dataset\n",
        "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ],
      "metadata": {
        "id": "xfDVwQVl39El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check all of the file names in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "id": "o4jaCEOd4DiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "Jk6gOdy34NAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create function to read the lines of a document\n",
        "def get_lines(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        return f.readlines()"
      ],
      "metadata": {
        "id": "RZZgSNwb4P_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines = get_lines(data_dir + \"trai.txt\")\n",
        "train_lines[:20]"
      ],
      "metadata": {
        "id": "92ymvKT84il3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text_with_line_numbers(filename):\n",
        "    input_lines = get_lines(filename) # get all lines form filename\n",
        "    abstract_lines = \"\" # create an empty abstract\n",
        "    abstract_samples = [] # create an empty list og abstract\n",
        "\n",
        "    # loop through each line in target file\n",
        "    for line in inputs_lines:\n",
        "        if line.stratswith('###'): # check to see if line is an ID line\n",
        "            abstract_id = line\n",
        "            abstract_liens = \"\" # reset abstract string\n",
        "        elif line.issapce(): # check to see if line is a new line\n",
        "            abstract_line_split = abstract_lines.splitlines() # split abstract into seaprate liens\n",
        "\n",
        "            # Iterate through eacth line in absract and count them at the same time\n",
        "            for abstract_line_number, abstract_lin in enumerate(abstract_line_split):\n",
        "                line_data = {} # create empty dict to stroe data from line\n",
        "                target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "                line_data[\"target\"] = target_text_split[0] # get target label\n",
        "                line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "                line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "                line_data[\"totalt_lines\"] = len(abstract_line_split) - 1 # how may total lines are in the abstract? (start from 0)\n",
        "                abstract_samples.appen(line_data) # add line data to abstract samples list\n",
        "\n",
        "        else: # if the above conditions aren't fulfilled, the line contrains labelled sentecne\n",
        "            abtsract_lines += line\n",
        "\n",
        "    return abstract_samples\n",
        "\n"
      ],
      "metadata": {
        "id": "p7tfSzsW4tFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bet data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + 'test.txt')\n",
        "len(train_samples), len(val_samples), len(test_samples)"
      ],
      "metadata": {
        "id": "rF_d2ZJ571MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the fisrt abstract of our training data\n",
        "train_samples[:14]"
      ],
      "metadata": {
        "id": "PWwxsH308eOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFram(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "PciLMnDf8kyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of labels in training data\n",
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "id": "ei5N1DPW9oPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.total_lines.plot.hist()"
      ],
      "metadata": {
        "id": "ePff7b1A9xAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get list of sentences"
      ],
      "metadata": {
        "id": "vwQREpK690Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert abstract text lines into list\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df['text'].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ],
      "metadata": {
        "id": "UnvQbfZl-AZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View first 10 lines of training senteces\n",
        "train_sentences[:10]"
      ],
      "metadata": {
        "id": "P64XhNtU-oe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make numeric labels (ML models require numeric labels)\n"
      ],
      "metadata": {
        "id": "inJZPfy0-vjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encode labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_one_hot"
      ],
      "metadata": {
        "id": "inV58s4U-40G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label encode labels"
      ],
      "metadata": {
        "id": "mzY6Z2P__rUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract labels (\"target\" columns) and encode them into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test-df[\"target\"].to_numpy())\n",
        "\n",
        "# Check what training labels look like\n",
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "clfpKv3L_uWk",
        "outputId": "bd39a8a8-216c-4d66-d302-6e442110e018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-558b7d0fb00a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_labels_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mval_labels_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class nsames and number of class from labelEncoder instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ],
      "metadata": {
        "id": "ljMPk92yAUof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a series of model experiments"
      ],
      "metadata": {
        "id": "rq2iYnrxAg5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Getting a baseline"
      ],
      "metadata": {
        "id": "b8oqiqOEAki2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "model_0 = Pipeline([\n",
        "    (\"tf-idf\", TfidVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(X=train_sentences,\n",
        "            y=train_labels_encoded);"
      ],
      "metadata": {
        "id": "cRSvoR-mAt9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evalutate baseline on validation dataset\n",
        "model_0.score(X=val_sentences,\n",
        "              y=val_labels_encoded)"
      ],
      "metadata": {
        "id": "3fMAtU9VAJq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds"
      ],
      "metadata": {
        "id": "rWKh2yc-Bltr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download helper function script\n"
      ],
      "metadata": {
        "id": "BQgEcdqEBsT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download helper functions script\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "id": "OGPlZePyByfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import calculate_results helper function\n",
        "from helper_functions import calculate_results"
      ],
      "metadata": {
        "id": "RalGTb15B2Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate baseline results\n",
        "baseline_results = calculate_results(y_true = val_labels_encoded,\n",
        "                                     y_pred = baseline_preds)\n",
        "\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "Gfxi5btLB-fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing our data for deep sequence models"
      ],
      "metadata": {
        "id": "fHGBjJoMCMIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "dT40t8MGCSmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long is each sentence on average?\n",
        "sent_lens = [len(sentence.split()) for sentence in train_senteces]\n",
        "avg_sent_len = np.mean(sent_lens)\n",
        "avg_sent_len # return average setence length (in tokens)"
      ],
      "metadata": {
        "id": "uwKYCPE0CY6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the distribution look like?\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_lens, bins = 7)"
      ],
      "metadata": {
        "id": "jxRcj30lCrv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How Long ot a sentence covers 95% of the lengths?\n",
        "output_seq_len = int(np.percentiles(sent_lens, 95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "id": "myP9GYKUCzbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maximum sentence lenth in the training set\n",
        "max(sent_lens)"
      ],
      "metadata": {
        "id": "4EiQWG1wC-ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create text vectorizer"
      ],
      "metadata": {
        "id": "IvL86dPpVZdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words are in our vocabulary?\n",
        "max_tokens = 68000"
      ],
      "metadata": {
        "id": "IL_fr0JzCQOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text vectorizer\n",
        "\n",
        "# After TensorFlow 2.6\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokesn, #number of word in vocabulary\n",
        "                                    output_sequence_length=55) # desired output length of vectorized sequences"
      ],
      "metadata": {
        "id": "AUquPRxOVqiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt text vectorizer to training sentences\n",
        "text_vectorizer.adat(train_setences)"
      ],
      "metadata": {
        "id": "7TkEwT8-WEOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out text vectorizer\n",
        "import random\n",
        "target-sentece = random.choice(train_sentences)\n",
        "print(f\"Text : \\n{target_sentence}\")\n",
        "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
        "print(f\"\\nVectorized text:\\n {text_vectorizer([target_sentence])}\")"
      ],
      "metadata": {
        "id": "9VX0cnmdWK3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many words in our training vacabulary?\n",
        "rct_20k_text_voca = text_vectorizer.get_vocabulary()\n",
        "print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\")\n",
        "print(f\"Most common words in the vocabulary : {rct_20k_text_vocab[:5]}\")\n",
        "print(f\"Least common words in the vocabulary : {rct_20k_text_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "vEVEa8M3WhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the config of our text vectorizer\n",
        "text_vectorizer.get_config()"
      ],
      "metadata": {
        "id": "xniyO2R_ZRsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create custom text embedding"
      ],
      "metadata": {
        "id": "txsFT1okZW-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create toekn embedding layer\n",
        "tocken_embed = laeyrs.Embedding(input_dim = len(rct_20k_text_vocab), # length of vocabulary\n",
        "                                output_dim=128, # Note : different embeding size result in drasticaaly different numbers of parameters to train\n",
        "                                # Use masking to handle variable sequence lengths (save space)\n",
        "                                mask_zero = True,\n",
        "                                name=\"token_embedding\")\n",
        "# Show example embedding\n",
        "print(f\"Sentence before vectorization : \\n {target_sentence}\\n\")\n",
        "vectorized_sentence = text_vectorizer([target_sentence])\n",
        "print(f\"Sentence after vectorization (before embedding) : \\n{vectorized_sentence}\\n\")\n",
        "embedded_sentence = token_embed(vectorzed_sentence)\n",
        "print(f\"Sentence after embedding : \\n {embedded_sentence}\\n\")\n",
        "print(f\"Embedded sentence shape : {embedded_sentence.shape}\")"
      ],
      "metadata": {
        "id": "f2AE8SgmZZzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create datasets (as fast as possible)"
      ],
      "metadata": {
        "id": "pjtwc77jbLg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into tensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slice((train_senteces, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slice((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slice((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "UJEM7XRMbQYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the TensorSliceDataset's and turn them into prefectched batches\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "xE2doIW1b1Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 : Conv1D with token embeddings"
      ],
      "metadata": {
        "id": "N9TaA-jgcNi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 1D convolutional model to process sequences\n",
        "inputs = layers.Input(shape = (1,), dtype = tf.string)\n",
        "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
        "token_embeddings = token_embed(text_vectors) # create embedding\n",
        "x = layers.Conv1D(64, kernel_size = 5, padding = \"same\", activation = \"relu\")(token_embeddings)\n",
        "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
        "outputs = layers.Dense(num_classes, activation = \"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile\n",
        "model_1.compile(loss = \"categorical_crossentorpy\", # if your labels are integer form (ont one hot) use sparse_categorical_crossentropy\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = [\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "xqVfjYCTcSsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get summary of Conv1D model\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "HVNJGMPMdeYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_dataset,\n",
        "                              steps_per_epoch = int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training itme\n",
        "                              epoch = 3,\n",
        "                              validation_data = valid_dataset,\n",
        "                              validation_steps = int(0.1 * len(vali_dataset))) # onlyvalidate on 10% of batches"
      ],
      "metadata": {
        "id": "0P1tUnlvkGNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on whole validation dataset( se only validated on 10% ot batches during training)\n",
        "model_1.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "jheg5_pVkfVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (out model outputs predicion porbabililites for each clss)\n",
        "model_1_pred_probs = model_1.predict(valid_dataset)\n",
        "model_1_pred_probs"
      ],
      "metadata": {
        "id": "hurMDHUNkrHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert pred probs to classes\n",
        "model_1_preds = tf.argmax(model_1_pred_probs, axis =1)\n",
        "model_1_preds"
      ],
      "metadata": {
        "id": "j6cAZefYk4DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_1 results\n",
        "model_1_results = calculate_results(y_true = val_labels_encoded,\n",
        "                                    y_pred = model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "nPBig5M3k__S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Feature extraction with pretrained token embeddings"
      ],
      "metadata": {
        "id": "7BQuOwQtlMTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Donwload pretrained TensorFlow Hub USE\n",
        "import tensorflow_hub as hub\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        trainbale = False,\n",
        "                                        name=\"universal_sentence_encoder\")"
      ],
      "metadata": {
        "id": "1wYLXrQXlPxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the embedding on a random sentece\n",
        "random_training_sentence  = random.choice(train_senteces)\n",
        "prin(f\"Random Training sentence : \\n {random_training_sentence}\")\n",
        "use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence])\n",
        "print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]} (truncated output)...\\n\")\n",
        "print(f\"Length of sentece embedding:\\n {len(use_embedded_sentence[0])}\")"
      ],
      "metadata": {
        "id": "9F3sZhC5lpP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and fitting and NLP feature extraction model from TensorFlow Hub"
      ],
      "metadata": {
        "id": "iWYYqOcemOV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature extractor model using TF Hub layer\n",
        "inputs = layers.Input(shape =[], dtype = tf.string)\n",
        "pretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding\n",
        "x = layers.Dense(128, activation = \"Relu\")(pretrained_embedding) # add a fully connected layer on top of the embeddig\n",
        "# Note : you could add more layers here if you wanted to\n",
        "outputs = layers.Dense(5, activaiton = \"softmax\")(x)# create the output layer\n",
        "model_2 = tf.keras.Model(inputs =inputs, outputs = outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = [\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "EZ8FBlAFmUv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a aummar ot the model\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "DgoPzp0tnK6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit feature extractor model for 3 epochs\n",
        "model_2.fit(train_dataset,\n",
        "            steps_per_epoch = int(0.1 * len(train_dataset)),\n",
        "            epochs = 3,\n",
        "            validation_data = valid_dataset,\n",
        "            validation_steps = int(0.1 * len(valid_dataset)))"
      ],
      "metadata": {
        "id": "BOjDTWKdnOa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on whole validation dataset\n",
        "model_2.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "cRER-KH9nfMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with feature extraction model\n",
        "model_2_pred_probs = model_2.predict(valid_dataset)\n",
        "model_2_pred_probs"
      ],
      "metadata": {
        "id": "4AS-gP2PnlCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the predictions with feature extraction model to classes\n",
        "model_2_preds = tf.argmax(model_2_pred_probs, axis =1)\n",
        "model_2_preds"
      ],
      "metadata": {
        "id": "gZ4x5A2KntXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results from TF Hub pretrained embeddings results on validaion set\n",
        "model_2_results = calculate_results(y_true=val_labels_encpded,\n",
        "                                    y_pred=model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "4C414uSNn3qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Conv1D with character embeddings"
      ],
      "metadata": {
        "id": "-4X0twj9oIaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a character-level tokenizer"
      ],
      "metadata": {
        "id": "iEXlLqTyoMD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make function to split sentences into characters\n",
        "def split_char(text):\n",
        "    return \" \".join(list(textm))\n",
        "\n",
        "# Test splitting non-character-level sequence into characters\n",
        "split_chars(random_training_setence)"
      ],
      "metadata": {
        "id": "Go2-fSA8oQgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_senteces]\n",
        "val_chars = [split_chars(sentence) for sentece in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "print(train_chars[0])"
      ],
      "metadata": {
        "id": "UXo1uvroopnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the average character lenth?\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_cah_len = np.mean(char_lens)\n",
        "mean_char_len"
      ],
      "metadata": {
        "id": "iv_xOf62o-8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ehck the distribution fo our sequences at character-level\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(char_lens, bins=7);"
      ],
      "metadata": {
        "id": "NtrGAOf8pKiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find what character lenth covers 95% of sequences\n",
        "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
        "output_seq_char_len"
      ],
      "metadata": {
        "id": "WFEX6ofTpSnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all keyboard characters for char-level embedding\n",
        "import string\n",
        "alphabet = string.ascii_lowercase +string.digits + straing.punctuation\n",
        "alphabet"
      ],
      "metadata": {
        "id": "N8DrOAFUpbsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char-level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) +  # num characters in alphabet + space + OOV token\n",
        "char_vectorizer = TextVectorization(max_tokens = NUM_CHAR_TOKENS,\n",
        "                                    output_sequence_length = output_seq_char_len,\n",
        "                                    standardize = \"lower_ans_strip_puctuation\",\n",
        "                                    name = \"char_vectorizer\")\n",
        "\n",
        "# Adapt character vectorizer to training characters\n",
        "char_vectorizer.adapt(train_chars)"
      ],
      "metadata": {
        "id": "KPiFV5d0pojH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check character vocabulary characteristics\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"number of differenct characters in character vocab \" {len(char_vocab)})\n",
        "print(f\"5 most common characters:{char_vocab[:5]}\")\n",
        "print(f\"5 least common characters : {char_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "j_KiHNreqJB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out character vectorizer\n",
        "random_train_chars = random.choie(train_chars)\n",
        "print(f\"charified text:\\n {tandom_train_chars}\")\n",
        "print(f\"\\nLength of chars : {len(random_train_chars.split())}\")\n",
        "vectorized_chars = char_vectorizer([random_train_chars])\n",
        "print(f\"\\nVectorized chars:\\n {vectorized_chars}\")\n",
        "print(f\"\\nLength of vecotrized chars:{len(vectorized_chars[0])}\")"
      ],
      "metadata": {
        "id": "MYm550vLqgcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creting a character-level embedding"
      ],
      "metadata": {
        "id": "7ntV8lpxofgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char embedding layer\n",
        "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKESN, # number of different characters\n",
        "                              output_dim=25, # embedding dimension of each character\n",
        "                              mask_zero=False, # don't use masks (this messes up model_5 if set to True\n",
        "                              name=\"char_embed\"\n",
        "                              )\n",
        "\n",
        "# Test out character embedding layer\n",
        "print(f\"Charified text (before vectorization and embedding):\\n{random_train_cahrs}\")\n",
        "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
        "print(f\"Embedded chars (after vectorization andembedding) : \\n{char_embed_example}\\n\")\n",
        "print(f\"Character embedding shape: {char_embed_exampled.shape}\")"
      ],
      "metadata": {
        "id": "7PfpeISLrFw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Builind a Conv1D model to fit on character embeddings"
      ],
      "metadata": {
        "id": "ZMFxhymmsTcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Conv1D on chars only\n",
        "inputs = layers.Input(shape = (1,), dtype =\"string\")\n",
        "char_vectors = chr_vectorizer(inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = layers.Conv1D(64, kernel_size = 5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(num_classes, activation = \"softmax\")(x)\n",
        "model_3 = tf.keras.Model(inputs =inputs,\n",
        "                         outputs = outputs,\n",
        "                         name=\"model_3_conv1D_char_embedding\")\n",
        "\n",
        "# Compile model\n",
        "model_3.compile(loxx=\"categorical_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrica=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "wGoEflalsYgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the summary of conv1d_char_model\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "mn0hSoretNqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char daatasets\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefectch(tf.data.AUTOTUNE)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_char_dataset"
      ],
      "metadata": {
        "id": "QLuI2yeXtTB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on chars only\n",
        "model_3_history = model_3.fit(train_char_dataset,\n",
        "                              steps_per_epoch = int(0.1 * len(train_char_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data = val_char_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_dataset)))"
      ],
      "metadata": {
        "id": "Ucu66fClttgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model_3 on whole validation char dataset\n",
        "model_3.evaluate(val_char_dataset)"
      ],
      "metadata": {
        "id": "_7dLqDlluCyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with character model only\n",
        "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
        "model_3_pred_probs"
      ],
      "metadata": {
        "id": "zlncdi7iuHuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction to classes\n",
        "model_3_preds = tf.argmax(model_3_pred_probs, axis = 1)\n",
        "model_3_preds"
      ],
      "metadata": {
        "id": "G9eIHkeWuQym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ccaluate Conv1D char only model results\n",
        "model_3_results = calculate_results(y_true = val_labels_encded,\n",
        "                                    y_pred = model_3_preds)\n",
        "model_3_results"
      ],
      "metadata": {
        "id": "sNEMSw2auZ3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4: Combining pretrained token embeddings + character embeddings (hybrid embedding layer)"
      ],
      "metadata": {
        "id": "Nu8fIceMxC26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup token inputs/model\n",
        "token_inputs= layers.Input(shape=[], dtype = tf.string, name=\"token_input\")\n",
        "token_embeddings =tf_hub_embedding_layer(toen_inputs)\n",
        "token_output = layers.Dens(128, activaion=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs = token_inputs,\n",
        "                             outputs = token_output)\n",
        "\n",
        "# 2. Setup char inputs/model\n",
        "char_iputs = layers.Input(shape = (1,), dtype = tf.string, name='char_input')\n",
        "char_vectors = char_vertorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectorx)\n",
        "char_bi_lstm = laeyrs.Birdirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure\n",
        "char_model = tf.keras.Model(inputs =char_inputs,\n",
        "                            outputs = char_bi_lstm)\n",
        "\n",
        "# 3.Concatenate token and char inputs (create hybrid token embedding)\n",
        "token_char_concat = layers.Concatenate(name =\"token_char_hybrid\")([token_model.output,\n",
        "                                                                   char_model.output])\n",
        "# 4. Create output layers - addition fo dropout discussed in 4.2\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dens(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
        "\n",
        "# 5. Construct model with char and token inputs\n",
        "model_4 = tf.keras.Model(inputs = [token_model.input, cha_model.input],\n",
        "                         outputs = output_layer,\n",
        "                         name = \"model_4_token_and_char_embeddings\")"
      ],
      "metadata": {
        "id": "zp7Ve5oOxJqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get summary of token and character model\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "v5Wqyb5Jzbau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hybrid token and character model\n",
        "from tensorflow.keras.uitls import plot_model\n",
        "plot_model(model_4)"
      ],
      "metadata": {
        "id": "ewWLVK4Jzfvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token char model\n",
        "model_4.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer = tf.keras.optimizer.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "LwHvggW_zpKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combininb token and character data in to a `tf.data` dataset"
      ],
      "metadata": {
        "id": "FLBbNbfTz1rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine char and token into a dataset\n",
        "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
        "train_char_token_labels =tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
        "train_char_toekn_dataset = tf.data.Dataset.zip((ttain_char_token_data, train_char_token_labels)) # combine data and labels\n",
        "\n",
        "# Prefetch and batch train data\n",
        "train_cahr_token_dataset = train_char_token_dataset.batch(32).prefectch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Repeat same steps validation data\n",
        "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
        "val_char_teken_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_dataset = tf.data.Dataset.aip((val_char_token_data, val_char_token_labels))\n",
        "val_char_token_dataset = bal_char_token_dataset.batch(32).pretecth(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "qJxrVPLgz95F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out training char and token embedding daataset\n",
        "train_char_token_dataset, val_char_token_dataet"
      ],
      "metadata": {
        "id": "f56eSSTC1Fwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting a model on token and character-level sequences"
      ],
      "metadata": {
        "id": "lLMSJINx1M50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on tokens and chars\n",
        "model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters\n",
        "                              steps_per_epoch = int(0.1 * len(train_char_token_dataset)),\n",
        "                              epochs = 3,\n",
        "                              validation_data = val_char_token_dataset,\n",
        "                              validaion_steps = int(0.1 * len(val_char_token_dataset)))"
      ],
      "metadata": {
        "id": "Skhvx8vN1RX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the whole validaion dataset\n",
        "model_4.evaluate(val_char_token_dataset)"
      ],
      "metadata": {
        "id": "HiEjJqpH1qY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the tolen-character model hybird\n",
        "model_4_pred_probs = model_4.predict(val_char_token_dataset)\n",
        "model_4_pred_probs"
      ],
      "metadata": {
        "id": "xhDFdSpo1vPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trun prediction probabilites into prediction classes\n",
        "model_4_preds = tf.argmax(model_4_pred_probs, axis = 1)\n",
        "model_4_preds"
      ],
      "metadata": {
        "id": "jZeL68d317OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get results of token-char-hybrid model\n",
        "model_4_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_4_preds)\n",
        "model_4_results"
      ],
      "metadata": {
        "id": "6wUfJCo82FvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5: Transfer Learning with pretrained token embeddings +character embeddings + positional embeddings"
      ],
      "metadata": {
        "id": "LKEEqx8F2TmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect training dataframe\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "gQk_2hnc2aQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create positional embeddings"
      ],
      "metadata": {
        "id": "0pmFiSxc3Asm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many diffenrent line numbers are there?\n",
        "train_df[\"line_number\"].value_counts()"
      ],
      "metadata": {
        "id": "6KGwkqlMrixk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of \"line_number\" column\n",
        "train_df.line_number.plot.hist()"
      ],
      "metadata": {
        "id": "vnemGfFr3qBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors of our \"line_number\" column\n",
        "\n",
        "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpyt(),depth = 15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "test_line_number_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)"
      ],
      "metadata": {
        "id": "lLeV26lZ3ztv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check one-hoe encoded \"line_number\" feature samples\n",
        "train_line_numbers_one_hot.shape, train_line_numbers_one_hot[:20]"
      ],
      "metadata": {
        "id": "Q-CZpQ8z4ZH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many different numbers of lines are there?\n",
        "train_df[\"total_lines\"],values_counts()"
      ],
      "metadata": {
        "id": "p__cdcvW4nYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of total lines\n",
        "train_df.total_lines.plot.hist()"
      ],
      "metadata": {
        "id": "WOD2SCju4vBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the coverage of a \"total_lines\" value of 20\n",
        "np.percentile(train_df.total_lines, 98) # a value of 20 covers 98% of samples"
      ],
      "metadata": {
        "id": "7hnHvbyF41LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlwo to create one-hot-encoded tensors of our \"total_lines\" column\n",
        "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(),depth=20)\n",
        "val_total_lines_ne_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth = 20)\n",
        "\n",
        "# Check shape and samples of total lines one_hot tensor\n",
        "train_total_lines_one_hot.shape, train_total_lines_one_hot[:10]"
      ],
      "metadata": {
        "id": "qdr3hP4j5AxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a tribrid embedding model"
      ],
      "metadata": {
        "id": "Hr5QwbO35o1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Token inputs\n",
        "toekn_inputs = layers.Input(shape=[],dtype=\"string\", name=\"token_inputs\")\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Mode(inputs = token_inputs,\n",
        "                            outputs = token_outputs)\n",
        "\n",
        "# 2.Char inputs\n",
        "char_inputs = layers.Input(shape = (1,), dtype = \"string\", name=\"char_inputs\")\n",
        "char_vectorss  char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(32))(char_embeddings)\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Line number inputs\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype = tf.int32, name=\"line_number_input\")\n",
        "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "line_number_model = tf.keras.Model(inputs = line_number_inputs,\n",
        "                                   outputs=x)\n",
        "\n",
        "# 4. Totla lines inputs\n",
        "total_lines_inputs = layers.Inputs(shape(21,), dtype=tf.int32, name = \"total_lines_input\")\n",
        "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "total_line_model = tf.keras.Model(inputs=total_lines_inputs, outputs=y)\n",
        "\n",
        "# 5. Combine token and char embeddings into a hybrid embedding\n",
        "combined_embeddings = layers.Concatenate(name=\"token_char_hybrid_embedding\")([token_model.output,\n",
        "                                                                              char_model.output])\n",
        "z = layers.Dense(256, activation=\"relu\")(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# 6. Combine positional embeddings with combined token and char embeddings into a tribrid embedding\n",
        "z = layers.Concatenate(name=\"token_char_positional_embedding\")([line_number_model.output,\n",
        "                                                                total_line_model.output,\n",
        "                                                                z])\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(5, activation=\"softmax\", name=\"output_layer\")(z)\n",
        "\n",
        "# 8. Put together model\n",
        "model_5 = tf.keras.Model(inputs = [line_number_model.input,\n",
        "                                   total_line_model.input,\n",
        "                                   token_model,input,\n",
        "                                   char_model.input],\n",
        "                         outputs = output_layer)\n"
      ],
      "metadata": {
        "id": "cntlgCHc5uHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of our token, char and positional embedding model\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "WaF2Ea5N9E9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the token, char, positional embeddin model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_5)"
      ],
      "metadata": {
        "id": "-CMzb8Bj9LF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which layers of our model are trainable or not\n",
        "\n",
        "for layer in model_5.layers:\n",
        "    print(layer, layer.trainable)"
      ],
      "metadata": {
        "id": "thwlZyM39ULB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token, char, positional embedding model\n",
        "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=), # add label smoothing ( examples which are really confident get smoothed a little)\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "g3re45DI9mpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create tribrid embedding datasets and fit tribrid model"
      ],
      "metadata": {
        "id": "thNT3rFm-Ho6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation datasets (all four kinds of inputs)\n",
        "train_pos_char_token_data = tf.ldataDataset.from_tensor_slices((train_line_numbers_one_hot, # line numbers\n",
        "                                                                train_total_lines_one_hot, # total lines\n",
        "                                                                train_sentences, # train tokens\n",
        "                                                                train_chars)) # train chars\n",
        "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slicestrain_labels_one_hot # train labels\n",
        "train_pos_char_token-dataset = tf.data.Dataset.zip((train_pos_car_token_data,train_pos_char_token_labels))# combine data and labels\n",
        "train_pos_char_token-dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefectch appropriately\n",
        "\n",
        "# Validation dataset\n",
        "val_pos_char_token-data = tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,\n",
        "                                                              val_total_lines_one_hot,\n",
        "                                                              val_setences,\n",
        "                                                              val_cahrs))\n",
        "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slcies(val_labels_one_hot)\n",
        "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data,val_pos_char_token_labels))\n",
        "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n",
        "\n",
        "# Check input shapes\n",
        "train_pos_char_token_dataset, val_pos_char_token_dataset\n",
        ""
      ],
      "metadata": {
        "id": "meKzXhAq-MV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the token, char and positional embedding model\n",
        "history_model_5 = model_5.fit(train_pos_char_token_dataset,\n",
        "                              steps_per_epoch = int(0.1 *len(train_pos_char_token_dataset)),\n",
        "                              epochs = 3,\n",
        "                              validation_data = val_pos_char_token_dataset,\n",
        "                              validation_steps = int(0.1 * len(val_pos_char_token_dataset)))"
      ],
      "metadata": {
        "id": "6pcyX1oVAGY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with token-char_positional hybrid model\n",
        "model_5_pred_probs = model_5.predict(val_pos_char_token_dataset, verbose=1)\n",
        "model_5_pred_probs"
      ],
      "metadata": {
        "id": "lhhhsuy_Ahv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilitied into prediction classes\n",
        "model_5_preds+tf.argmax(model_5_pred_probs, axis=1)\n",
        "model_5_preds"
      ],
      "metadata": {
        "id": "bIxPxjUsAs0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate result of token-char_positional hybrid model\n",
        "model_5_results = calculate_results(y_true = val+lavels_encoded,\n",
        "                                    y_pred=model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "id": "7RNLFdTNA3Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare model results"
      ],
      "metadata": {
        "id": "4JYCX1qABEfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine model results inot a DataFrame\n",
        "all_model_results = pd.DataFrame({\"baseline\":baseline_results,\n",
        "                                  \"custom_token_embed_conv1d\":model_1_results,\n",
        "                                  \"pretrained_token_embed\":model_2_results,\n",
        "                                  \"custome_char_embed_con1d\":model_3_results,\n",
        "                                  \"hybrid_char_token_embed\":model_4_results,\n",
        "                                  \"tribrid_pos_char_token_embed\" : model_5_results})\n",
        "all_model_results = all_model_results.transpose()\n",
        "all_model_results"
      ],
      "metadata": {
        "id": "yDzj15gYBKtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the accuracy to same scale as other metrics\n",
        "all_model-results[\"accuracy\"] = all_model_results[\"accuracy\"]/100"
      ],
      "metadata": {
        "id": "wxHNmCsdBu8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and coampre all of the model results\n",
        "all_model_results.plot(kind=\"bar\", figsize = (10,7)).legend(bboax_to_anchor = (1.0, 1.0));"
      ],
      "metadata": {
        "id": "wfGiorTNB6Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort model results by f1-score\n",
        "all_model_results.sort_values(\"f1\", ascending=False)[\"f1\"].plot(kind=\"bar\",figsize=(10,7));"
      ],
      "metadata": {
        "id": "1JE2uhhy6rm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and load best performing model"
      ],
      "metadata": {
        "id": "mFEB-woKCaYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save best performing model to SavedModel formag (default)\n",
        "model_5.save(\"skimlit_tribrid_model\")# model will be save to path specified by string"
      ],
      "metadata": {
        "id": "RtBxKW2fCvBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download pretrained model from Google Storage\n",
        "!wget # 여기에 내 구글 드라이브 주소 넣기\n",
        "!mkdir skimlit_gs_model\n",
        "!unzip skimlit_tribrid_model.zip -d skimlit_gs_model"
      ],
      "metadata": {
        "id": "2pm6ckGDC8Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow model dependencies\n",
        "import tensorflow_hub as hub\n",
        "import tnesorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "model_path = \"skimlit_gs_model/skimlit_tribrid_model/\" # 내 디렉토리 주소\n",
        "\n",
        "# Load downloaded model from Google Storage\n",
        "loaded_model = tf.keras.models.lad_model(model_path)"
      ],
      "metadata": {
        "id": "ck9EmP6PDLu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions and evaluate them against the truth labels"
      ],
      "metadata": {
        "id": "bNkoKLE1Dm9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the loaded model on the validation set\n",
        "loaded_pred_probs = loaded_model.predict(val_pos_char_token_dataset,verbose = 1)\n",
        "loaded_preds = tf.argmax(loaded_pred_probs, axis = 1)\n",
        "loaded_preds[:10]"
      ],
      "metadata": {
        "id": "7wp7g3sHDq2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded model's predictions\n",
        "loaded_model_results= calculate_results(val_labels_encoded,\n",
        "                                        loaded_preds)\n",
        "loaded_model_results"
      ],
      "metadata": {
        "id": "te7Bx0laEiOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare loaded model results with original trained model results( should be quite clsoe)\n",
        "np.isclose(list(model_5_results.values()), list(loaded_model_results.values()), rtol=1e-02)"
      ],
      "metadata": {
        "id": "C82zfr1NEtP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check loaded model summary\n",
        "loaded_model.summary()"
      ],
      "metadata": {
        "id": "UjXuSD-HE9yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate model on test dataset"
      ],
      "metadata": {
        "id": "8ugnPZcDFGBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test dataset batch and prefetched\n",
        "test_pos_char_token_data = tf.data.Dataet.from_tensor_slices((test_line_numbers_one_hot,\n",
        "                                                              test_totla_lines_one_hot,\n",
        "                                                              test_sentences,\n",
        "                                                              test_chars))\n",
        "test_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
        "test_pos_char_token_dataset = tf.data.Dataset.zip((test_poes_char_token_data,test_pose_char_token_labels))\n",
        "test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Check shpae\n",
        "test_pose_char_token_dataset"
      ],
      "metadata": {
        "id": "nTfl7AbnFJb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test dataset\n",
        "test_preds_probs = loaded_model.predict(test_poes_char_token_dataset,\n",
        "                                        verbose=1)\n",
        "\n",
        "test_preds = tf.argmax(test_pred_probs, axis = 1)\n",
        "test_preds[:10]"
      ],
      "metadata": {
        "id": "2UCcW45UF1k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded model test predictions\n",
        "loaded_model_test_results= calcualte_reuslts(y_true=test_labels_encoded,\n",
        "                                             y_pred = tet_preds)\n",
        "loaded_model_test_results"
      ],
      "metadata": {
        "id": "fHlS0B_hHKq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find most wrong"
      ],
      "metadata": {
        "id": "5mttomTpHXQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Get list of class name of test predictions\n",
        "test_pred_classes = [label_encoder.classes_[pred] for pred in test_preds]\n",
        "test_pred_classes\n"
      ],
      "metadata": {
        "id": "xctJNTiwHZxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create prediction-enriched test dataframe\n",
        "test_df[\"prediction\"] = test_pred_classes # create column with test prediction class names\n",
        "test_df[\"pred_prob\"] = tf.reduce_max(test_pred_probs, axis = 1).numpy() # get the maxmum prediction probability\n",
        "test_df[\"correct\"] = test_df[\"prediction\"] == test_df[\"target\"] # create binary column for whether the prediciton is right or not\n",
        "test_df.head(20)"
      ],
      "metadata": {
        "id": "appJ9WIQHmXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 100 most wrong samples (note:100 is an abitrary number, you could go through al go them if you wanted)\n",
        "top_100_wrong = test_df[test_df[\"correct\"] == False].sort_values(\"pred_prob\", ascending=False)[:100]\n",
        "top_100_wrong"
      ],
      "metadata": {
        "id": "g3hW8C70I3qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Investigate top wrog preds\n",
        "for row in top_100_wrong[0:10].itertuples(): # adjust indexes to view different samples\n",
        "    _, target, test, line_number, total_lines, prediction, pred_prob, _ = row\n",
        "    print(f\"Tartget: {target}, Pred : {prediction}, Prob ;{pred_prob}, Line number : {line_number}, Total lines : {total_lines}\\n\")\n",
        "    print(f\"Text:\\n{text}\\n\")\n",
        "    print(\"-----\\n\")"
      ],
      "metadata": {
        "id": "GxZPAGbUJSEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make example predictions"
      ],
      "metadata": {
        "id": "mqnaeFv4J9bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Download and open example abstracts (copy and pasted from Pubmed)\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/skimlit_example_abstracts.json\n",
        "\n",
        "with open(\"skimilit_example_abstracts.json\", \"r\") as f:\n",
        "    example_abstracts = json.load(f)\n",
        "\n",
        "example_abstracts"
      ],
      "metadata": {
        "id": "2UvWtm-oKCkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See what our examples abstract look like\n",
        "abstracts = pd.DataFrame(example_abstracts)\n",
        "abstracts"
      ],
      "metadata": {
        "id": "xQSJ4g56KrW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sentencizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English() # setup English sentence parser\n",
        "\n",
        "# New version of spacy\n",
        "sentencizer = nlp.add_pipe(\"sentencizer\") # create sentence splitting pipeline object\n",
        "\n",
        "# Create \"doc\" of parsed sequenceds, chage index for a different abstract\n",
        "doc = nlp(example_abstracts[0][\"abstract\"])\n",
        "abstract_lines = [str(sent) for sent in list(doc.sents)] # return detected sentences from doc in string type (not spacy token type)\n",
        "abstract_lines"
      ],
      "metadata": {
        "id": "8dNDC9qdK1qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get total number of lines\n",
        "total_lines_in_sample = len(abstract_lines)\n",
        "\n",
        "# Go through each line in abstract and create a list of dictionaries containing features for each line\n",
        "sample_lines = []\n",
        "for i, line in enumcerate(abstract_lines):\n",
        "    sample_dict = {}\n",
        "    sample_dict[\"text\"] = str(line)\n",
        "    sample_dict[\"line_number\"] = total_lines_in_sample - 1\n",
        "    sample_lines.append(sample_dict)\n",
        "sample_lines"
      ],
      "metadata": {
        "id": "BKUILbtELiXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all line_number balues from sample abstract\n",
        "test_abstract_line_number = [line[\"line_number\"] for line in sample_lines]\n",
        "# One-hot encode to same depth as training data, so model accepts right input shape\n",
        "test_abstract_line_numbers_one_hot = tf.one_hot(test-abstract_line_numbers, depth=15)\n",
        "test_abstract_line_numbers_one_hot"
      ],
      "metadata": {
        "id": "vNQkRpx8MBrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all total_lines values from sample abstract\n",
        "test_abstract_total_lines = [line[\"total_lines\"] for line in sample_lines]\n",
        "# One-hot encoded to same depth as training data, so model accepts right input shape\n",
        "test_abstract_total_lines_one_hot = tf.one_hot(test_abstract_total_lines, depth=20)\n",
        "test_abstract_total_lines_one_hot"
      ],
      "metadata": {
        "id": "dErz77crMi-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split abstract line into characters\n",
        "abstract_chars = [split_char(sentece) for snetence in abstract_lines]\n",
        "abstract_chars"
      ],
      "metadata": {
        "id": "OFsM56XlM7_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on sample abstract features\n",
        "%%time\n",
        "test_abstract_pred_probs = loaded_model.predict(x=(test_abstract_line_numbers_one_hot,\n",
        "                                                   test_abstract_total_lines_one_hot,\n",
        "                                                   tf.constant(abstract_lines),\n",
        "                                                   tf.constant(abstract_chars)))\n",
        "\n",
        "test_abstract_pred_probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "CcMJoIioNGFi",
        "outputId": "579f9a39-e634-4c0f-867a-7a9ed70ccfb7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_abstract_pred_probs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_abstract_pred_probs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilities into prediction classes\n",
        "test_abstract_preds = tf.argmax(test_abstract_pred_probs, axis=1)\n",
        "test_abstract_preds"
      ],
      "metadata": {
        "id": "FH62vrZlP9wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn predcion class integers into string class names\n",
        "test_abstract_pred_classes = [label_encoder.classes_[i] for i in test_abstract_preds]\n",
        "test_abstract_pred_classes"
      ],
      "metadata": {
        "id": "U_j9NjaVQIgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vilualize abstract lines and predicted sequence labels\n",
        "for i, line in enumerate(abstract_lines):\n",
        "    print(f\"{test_abstract_pred_classes[i]} : {line}\")\n",
        ""
      ],
      "metadata": {
        "id": "pymrRz0ZQZgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n72ZwR_PQrJQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}