{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBfQjMPuxYdR"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "print(f\"Notebook last run (end-to_end) : {datetime.datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi-L"
      ],
      "metadata": {
        "id": "W-fWx0x2yAXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download helper functions script\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "id": "ignwKtnDyGzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import series of helper functions for the notebook\n",
        "from helper_function import unzip_data, create_tensorboard_callback, plot_loss_curves, campare_historys"
      ],
      "metadata": {
        "id": "LiiflxiwyJpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a test dataset"
      ],
      "metadata": {
        "id": "NHV0NgluyZci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data (same as from Kaggle)\n",
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n",
        "\n",
        "# Unzip data\n",
        "unzip_data(\"nlp_getting_started.zip\")"
      ],
      "metadata": {
        "id": "-4MjZFykycqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing a text dataset"
      ],
      "metadata": {
        "id": "GFd3_6zpys2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn .csv files into pandas DataFrame's\n",
        "\n",
        "import pandas as pd\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "FkSJRgpFy0ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle trainig dataframe\n",
        "train_df_shffled = train_df.smapel(frac = 1, random_state = 42) # shuffle with random_state = 42 for reproducibility\n",
        "train_df_shuffled.head()"
      ],
      "metadata": {
        "id": "tMlz1q1AzMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thes test data deosn't have a target (that's what we'd try to predict)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "PGsD1u31zd2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many exmapels of each calss?\n",
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "id": "S3bDtbsLzqgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How many sampels total?\n",
        "print(f\"Total training samples : {len(train_df)}\")\n",
        "print(f\"Total test samples: {len(test_df)}\")\n",
        "print(f\"Total smaples : {len(train_df) + len(test_df)}\")"
      ],
      "metadata": {
        "id": "XNHAimNmzy1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's visaulize some random training exmaples\n",
        "import random\n",
        "random_index = random.randint(0, len(train_df)-5) # Create random indexs not higher than the total number\n",
        "for row in train_df_shuffled[[\"text\",\"target\"]][random_index:random_index+5].itertuples():\n",
        "    _, text, target = row\n",
        "    print(f\"Target : {target}\", \"(real disaster)\" if target > 0  else \"(not real disaster)\")\n",
        "    print(f\"Text:\\n {text} \\n\")\n",
        "    print(\"---\\n\")"
      ],
      "metadata": {
        "id": "I1Vj3X000EdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data into trainig and validation sets"
      ],
      "metadata": {
        "id": "ykocCTaV1w6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import tran_test_split\n",
        "\n",
        "# Use train_test_split to split training data into training and validation sets\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shffled[\"text\"].to_numpy(),\n",
        "                                                                            train_df_shffled[\"target\"].to_numpy(),\n",
        "                                                                            test_size = 0.1m # dedicate 10 % of samples to validation set\n",
        "                                                                            random_state = 42) # random state for reproducibility"
      ],
      "metadata": {
        "id": "3M6FWJHc1bsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the lengths\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "metadata": {
        "id": "YOtOQ8oezvih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 10 training sentences and their labels\n",
        "train_sentences[:10], train_labesl[:10]"
      ],
      "metadata": {
        "id": "Cn8WOpyZ2zAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting text into numbers"
      ],
      "metadata": {
        "id": "HA7eKk3h26nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text vectorization (tokenization)"
      ],
      "metadata": {
        "id": "JgOQAjMx29gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tr\n",
        "from tensorflow.keras.lyaers import Textvectorization # after tensorFlow 2.6\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens = None,\n",
        "                                    standardize = \"lower_and_strip_punctuation\",\n",
        "                                    split=\"whitespace\",\n",
        "                                    ngrams = None, # create groups of n_words\n",
        "                                    output_mode = \"int\", # how to map tokens to number\n",
        "                                    output_sequence_length = None) # how long should the output sequence of token be?\n",
        ""
      ],
      "metadata": {
        "id": "axXqQ8s43oyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find average number of tokens (words) in training Tweets\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "metadata": {
        "id": "wppt13j74hWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup text vectorization with custom variables\n",
        "max_vacab_lenth = 1000 # max number of words to have in oour vocabulary\n",
        "max_lenths  = 15 # max lenth our sequences will be (e.gl how many words from a Tweet does our model see?)\n",
        "text_vectorizer = TextVectorizatioon(max_tokens=max_vocab_lenth,\n",
        "                                     output_mode = \"int\",\n",
        "                                     output_sequence_length = max_length)"
      ],
      "metadata": {
        "id": "7ahu74vV44sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the text vectorizer to the training text\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "1Mk7qUvz5dtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample sentence and tokenizer it\n",
        "sample_sentence = \"There's a flood in my street!\"\n",
        "text_vertorizer([sample_sentence])"
      ],
      "metadata": {
        "id": "DQD4joT75vFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a random sentence form thr trainig dataset and tokenize it\n",
        "random_setence = randoom.choice(train_sentences)\n",
        "print(f\"Original text : \\n{random_sentence}\\\n",
        "    \\n\\nVectorized version :\")\n",
        "text_vectorizer([random_sentence])"
      ],
      "metadata": {
        "id": "fGKx0CMd57P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique words in the vocabulary\n",
        "words_in_voca = text_vectorizer.get_vocabulary()\n",
        "top_5_words = words_in_vocab[:5] # most common token (notice  the [UNK] token for \"unknown\" words)\n",
        "botton_5_words = words_io_vocab[-5:] # least commen tokens\n",
        "print(f\"Number of words in vocab : {len(words_in_voca)}\")\n",
        "print(f\"Top 5 most common words : {top_5_words}\")\n",
        "print(f\"Botton 5 least common words : {bottom_5_words}\")"
      ],
      "metadata": {
        "id": "CdhL__vK6Xv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an Embedding using an Embedding Layer"
      ],
      "metadata": {
        "id": "M90Xv1PJ7P8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding = laeyrs.Embedding(input_dim = max_vocab_length, # set input shape\n",
        "                             output_dim = 128, # set size of embedding vector\n",
        "                             embeddings_initializer = \"uniform\", # default, initializer randomly\n",
        "                             input_length = max_length, # how long is each input\n",
        "                             name = \"embedding_1\"\n",
        "                             )"
      ],
      "metadata": {
        "id": "T6sN1X7H7Ueb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random sentece from training set\n",
        "random_sentence = random.choice(train_setences)\n",
        "print(f\"Original text: \\n{random_sentence}\\\n",
        "    \\n\\nEmbedded version :\")\n",
        "\n",
        "# Embed the random sentence (trun it into numberical representation)\n",
        "sample_embed = embedding(text_verorizer([random_sentence]))\n",
        "sampe_embed"
      ],
      "metadata": {
        "id": "BdZgh7vI77uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cCheck out a single token's embedding\n",
        "sample_embed[0][0]"
      ],
      "metadata": {
        "id": "B8JVAXcg8gL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0 : Getting a baseline"
      ],
      "metadata": {
        "id": "9Q8wRcbV8luu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidVectorizer\n",
        "from sklearn.naive_bayes import MultinminalNB\n",
        "from skelarn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "    (\"tfidf\", TfidVecotorizer()), # convert words to numbers using tfidf\n",
        "    (\"clf\", MultinominalNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)"
      ],
      "metadata": {
        "id": "1KkfHnfu89qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of : {baseline_score*100:.2f}%\")"
      ],
      "metadata": {
        "id": "wFHbTUq59yMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "baseline_pres = model_0.predict(val_sentences)\n",
        "baseline_preds[:20]"
      ],
      "metadata": {
        "id": "Zg68_Slp-EoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an evaluation fucntion froml our model experiments"
      ],
      "metadata": {
        "id": "RE-iJ5nY-McW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to eavluate : accuracy, precision, recall, f1-score\n",
        "from sklearn.metircs import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def caluate_results(y_true, y_pred):\n",
        "\n",
        "    # Calculate model accuracy\n",
        "    model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "    # Calulate model precision, recall and f1 score using \"weighted\" average\n",
        "    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average = \"weighted\"\n",
        "    model_results = {\"accuracy\" : model_accuracy,\n",
        "                     \"precision\" : model_precision,\n",
        "                     \"recall\" : model_recall,\n",
        "                     \"f1\" : model_f1})\n",
        "    return model_results"
      ],
      "metadata": {
        "id": "58T2R9r5-T_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get baseline results\n",
        "baseline_results = caluate_results(y_true=val_labels,\n",
        "                                   y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "id": "9-gtlfY6_Tjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 : A simpole dense model\n"
      ],
      "metadata": {
        "id": "iF39vFiU_jPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensorboard callback (need to create a new one for each model)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "# Create directory to save TensorBoard logs\n",
        "save_dir = \"model_logs\""
      ],
      "metadata": {
        "id": "giaz1EQV_nnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model with the Functional API\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype = \"String\") # inputs are 1-dimensional strings\n",
        "x = text_vectorizer(inputs) # turn the input text into number\n",
        "x = embedding(x) # create an embeddingof the numerized numbers\n",
        "x = layers.GlobalAveragePooling1D()(x) # create the output layer, want binary outputs so use igmoid activaion\n",
        "model_1 = tf.keras.Model(inputs, outputs, name = \"model_1_dense\") # construct the model"
      ],
      "metadata": {
        "id": "uG8IyuenAFyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model_1.compile(loss = \"binary_crossentropy\",\n",
        "                optmizer=tf.keras.optimizers.Adam().\n",
        "                metrics=[\"accracy\"])"
      ],
      "metadata": {
        "id": "X0W_RRjrA9Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sammar of the model\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "f0WmML0QDoFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train__sentences, # input senteces can be a list of strings due to next prprocessing layer built-in model\n",
        "                              train_labels,\n",
        "                              epochs -=5,\n",
        "                              validation_data = (val_senteces, val_blabels),\n",
        "                              callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR,\n",
        "                                                                       experiment_name = \"simple_dense_model\")])\n",
        ""
      ],
      "metadata": {
        "id": "HSWN-tMxDsCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the results\n",
        "model_1.evaluate(val_senteces, val_labels)"
      ],
      "metadata": {
        "id": "u2bIIRGiENCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.weights"
      ],
      "metadata": {
        "id": "tHCbtJjcETPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_weighs = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
        "print(embed_weights.shape)"
      ],
      "metadata": {
        "id": "JAqX2NtHEUmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # View tensorboard logs of transfer learning modelling experiments (should be 2 models)\n",
        "# # Upload tensorBoard dev records\n",
        "# !tensorboard dev upload --logdir ./model_logs\\\n",
        "# --name \"First deep model on text data\" \\\n",
        "# --description \"Trying a dense model with an embedding layer\" \\\n",
        "# --one_shot # exits the uploader when upload has finished"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojVcdwOkEewA",
        "outputId": "c2b783f5-2550-40ae-9b9e-6997a1ccdffc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-03 04:36:55.062896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748925415.099288     495 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748925415.110730     495 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-03 04:36:55.151412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "======================================================================\n",
            "ERROR: The `tensorboard dev` command is no longer available.\n",
            "\n",
            "TensorBoard.dev has been shut down. For further information,\n",
            "see the FAQ at <https://tensorboard.dev/>.\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction s(these come back in the form of probabilites)\n",
        "model_1_pred_probs = model_1.predict(val_sentences)\n",
        "model_1_pred_probs[:10] # only print out the first 10 prediction probabilites"
      ],
      "metadata": {
        "id": "RcWiV_tBFKxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilites into single-dimension tensor of floats\n",
        "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze remove single dimensions\n",
        "model_1_preds[:20]"
      ],
      "metadata": {
        "id": "8a6SyPJUFfjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caluate model_1 metircs\n",
        "model_1_results = calculate_results(y_true = val_labels,\n",
        "                                    y_pred = model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "yfTowcZRF-Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Is our simple Keras model better than our baseline model?\n",
        "import numpy as np\n",
        "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))\n"
      ],
      "metadata": {
        "id": "xH-eaCiuGK0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a helper function to compare our baseline results to new model results\n",
        "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
        "    for key, value in baseline_results.items():\n",
        "        print(f\"Baseline {key} : {value:.2f}, New {key} : {new_model_results[key]:.2f}, Difference : {new_model_results[key]-value:.2f}\")\n",
        "\n",
        "compare_baseline_to_new_results(baseline_results=baseline_results,\n",
        "                                new_model_results = model_1_results)"
      ],
      "metadata": {
        "id": "xdGkOLm0GZ8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing learned embeddings"
      ],
      "metadata": {
        "id": "umsL5U3pHMWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vocaublary from the text vetorization layer\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "len(words_in_vocab), words_in_vocab[:10]"
      ],
      "metadata": {
        "id": "qbXKhY7yHPTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.summary()"
      ],
      "metadata": {
        "id": "gk2r9x8mHnEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the wight matix of embeddinglayer\n",
        "# (these are the numerical patterns between the text in the trainging dataset the model has learned)\n",
        "embed_weights  = model_1.get_layer(\"embedding_1\").get_weights()[0]\n",
        "print(embed_weights.shape) # same size as vocab size and embedding_dim (each word is a embedding_dim size vector)"
      ],
      "metadata": {
        "id": "yjoQiNVPHqF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code below is adapte from :\n",
        "import io\n",
        "\n",
        "# Create ouptut writers\n",
        "out_v = io.open(\"embedding_vectors.tsv\", \"w\", encoding= \"utf-8\")\n",
        "out_m = io.open(\"embedding_metadata.txv\", \"w\", encoding=\"utf-8\")\n",
        "\n",
        "# Write embedding vectors and words to file\n",
        "for num, word in enumerate(words_in_vocab):\n",
        "    if num == 0:\n",
        "        continue # skip padding token\n",
        "        vec = embed_weights[num]\n",
        "        out_m = embed_weights[num]\n",
        "        out_m.write(word+'\\n') # write words to file\n",
        "        out_v.write(\"\\t\".join([str(x) for x in vec]) + '\\n') # write corresponding word vector to file\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "# Download files locally to upload to Embedding projector\n",
        "try:\n",
        "    from google,lolab import files\n",
        "except ImportError:\n",
        "    pass\n",
        "else:\n",
        "    files.download(\"embedding_vectors.tsv\")\n",
        "    files.download(\"embedding_metadata.tsv\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "PQkzAFirItis",
        "outputId": "4c54b896-562f-4852-dfec-07f064e217ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "unknown encoding: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1ac2b76848a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create ouptut writers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mout_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding_vectors.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embedding_metadata.txv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m: unknown encoding: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNN's)"
      ],
      "metadata": {
        "id": "wEEOCAKDKKp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: LSTM"
      ],
      "metadata": {
        "id": "VZuiLBc3KYit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set random seed and create embedding laeyr (new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model_2_embedding =layers.Embedding(input_dim=ax_vocab_length,\n",
        "                                    output_dim = 128,\n",
        "                                    embeddings_initializer = \"uniform\",\n",
        "                                    input_length = max_length,\n",
        "                                    name = \"embedding_22\"\n",
        "                                    )\n",
        "# create LSTM model\n",
        "inputs = layers.Input(shape=(1,), dtype = \"string\"\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_2_embedding(x)\n",
        "print(x.shape)\n",
        "# x = layers.LSTM(64, return_sequences = True)(x)# return vector for each word in the Tweet ( you can stack RNN cell as long as return_wequences = True)\n",
        "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
        "print(x.shape)\n",
        "x = layers.dense(64, activation = \"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
        "outputs = layers.Dense(1, activiation=\"sigmoid\")(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs, name = \"model_2_LSTM\")\n"
      ],
      "metadata": {
        "id": "0P9akmj7FG0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model_2.compile(loss = \"binary_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "HBiYphIeNL5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.summary()"
      ],
      "metadata": {
        "id": "CPMhFnKINeov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "model_2_history = model_2.fit(train_senteces,\n",
        "                              train_labels,\n",
        "                              epochs = 5,\n",
        "                              validation_data = (val_sentences, val_labels),\n",
        "                              callbacks = [Create_tensorboard_callback(SAVE_DIR,\"LSTM\")])"
      ],
      "metadata": {
        "id": "Rq3kijCdNgNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation dataset\n",
        "model_2_pred_ptrobs = model_2.predict(val-senteces)\n",
        "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
      ],
      "metadata": {
        "id": "aNj0VHe_N24E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round out predictions and reduce to 1-dimensional array\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_w_preds[:10]"
      ],
      "metadata": {
        "id": "Thp3rddlOHZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate LSTM model results\n",
        "model_2_results = calculate_results(y_true = val_labels,\n",
        "                                    y_pred= model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "7NSnSZar-Phv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare model 2 to baseline\n",
        "compare_baselint_to_new_results(baseline_results, model_2_results)"
      ],
      "metadata": {
        "id": "o2EdB25jOvrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_3 : GRU"
      ],
      "metadata": {
        "id": "xLv08UzmPXK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed and create embeddin layer ( new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "from tensoflow.keras import layers\n",
        "model_3_embedding = layers.Embedding(input_dim=max_vocab_length,\n",
        "                                     ouput_dim=128,\n",
        "                                     embeddings_initailiaer = \"uniform\",\n",
        "                                     input_length = max_length,\n",
        "                                     name = \"embdding_3\")\n",
        "\n",
        "# Build on RNN using the GRU cell\n",
        "inputs = layers.Input(shape=(1,), dtype = \"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_3_embedding(x)\n",
        "x = layes.GRU(63, return_sequeces = True) # stacking recurrent cesll requires return_sequences = True\n",
        "x = layers.GRU(64)(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\n",
        "ouputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
        "model_3 = tf.keras.Mode(inputs, outputs, name = \"model_3_GRU\")"
      ],
      "metadata": {
        "id": "sSl1Ev1gPckh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile GRU model\n",
        "model_3.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer= tf.keras.optimizers.Adam(),\n",
        "                metrics =[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "--q5jXoaPlCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary ot the GRU model\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "v1yi__Po8RUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model\n",
        "model_3_history = model_3.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks = [create_tensorboard_callback(SAVE_DIR,\"GRU\")])"
      ],
      "metadata": {
        "id": "97yevc_KRaqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation data\n",
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "model_3_pred_probs.shape, model_3_pred_probs[:10]"
      ],
      "metadata": {
        "id": "W3kYoin8RucP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction probabilites to prediction classes\n",
        "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
        "model_3_preds[:10]"
      ],
      "metadata": {
        "id": "1lZtwtQxR7Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate model_3 results\n",
        "model_3_results = caluate_results(y_true = val_labels,\n",
        "                                  y_pred = model_3_preds)\n",
        "model_3_results"
      ],
      "metadata": {
        "id": "BkT9Z1qWSJs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare to baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_3_results)"
      ],
      "metadata": {
        "id": "3O_-dwxlSXDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 4 : Bidirectional RNN model"
      ],
      "metadata": {
        "id": "aoWzaerLSfN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed and create embedding layer ( new embedding layuer for each model)\n",
        "tf,random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model_4_embedding = layers.Embedding(input_dim_vax_vocab_length, output_dimr = 128,\n",
        "                                     embeddings_initializer = \"uniform\",\n",
        "                                     input_length = max_length,\n",
        "                                     name = \"embedding_4\")\n",
        "\n",
        "# Build a Bidirectional RNN in TensorFlow\n",
        "inputs = layers.Input(shape = (1,), type = \"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_4_embedding(x)\n",
        "# x = layers.Bidirectional(layers.LSTM(64, return_sequences = True))(x) # stacking RNN layers requires return_sequences = Ture\n",
        "x = layers.Dense(1, acitivation =\"sigmoid\")\n",
        "ouputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
        "model_4 = tf.keras.Model(inputs, outputs, name = \"model_4_Bidirectional\")\n"
      ],
      "metadata": {
        "id": "t1hoDfN_SiB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "model_4.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "iWnYo4BBT80m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a summary of our bidirectional model\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "RtsdfOFhUPII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model (takes longer becaus of the bidirectional layers)\n",
        "model_4_history = model_4.fit(train_sentecnes,\n",
        "                              train_labels,\n",
        "                              epochs = 5,\n",
        "                              validation_data = (val_sentences, val_labels)),\n",
        "                              callbacks = [create_tensorboard_callback(SAVE_DIR,\"bidirectional_RNN\")]"
      ],
      "metadata": {
        "id": "ZvYBpMYVUSS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make prediction swith bidirectional RNN on the validation data\n",
        "model_4_pred_probs = model_4.predict(val_sentences)\n",
        "model_4_pred_probs[:10]"
      ],
      "metadata": {
        "id": "M_jin9sJUqlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions probabilites to labels\n",
        "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ],
      "metadata": {
        "id": "lkX9D2l_U36Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Caluate bidirectional RNN model results\n",
        "model_4_results = calculate_results(val_labels, model_4_preds)\n",
        "model_4_results"
      ],
      "metadata": {
        "id": "sxS6vcLZVHdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to see how the bidirectional model performs against the baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_4_results)"
      ],
      "metadata": {
        "id": "9iE93GqyVUX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Networks for Text"
      ],
      "metadata": {
        "id": "afYWgadBVeTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 5: Conv1D"
      ],
      "metadata": {
        "id": "kh4hJV3PVkdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the embedding, 1D convolutional and max pooling\n",
        "embedding_test = embedding(text_vectorizer([\"this is a test sentece\"])) # turn target sentece into embedding\n",
        "conv_1d = layers.Conv1D(filters = 32, kernel_size = 5, activaion=\"relu\") # convolve over target sequence 5 words at a time\n",
        "conv_1d_output = conv_1d(embeding_test) # pass embedding through 1D convolutional layer\n",
        "max_pool = layers.GlobalMaxPooling1D()\n",
        "max_pool_output = max_pool(conv_1d_output)# get the most important features\n",
        "embedding_test.shape, conv_1d_output.shape, max_pool_output.shape"
      ],
      "metadata": {
        "id": "2x6DGfDjV6L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the outputs of each layer\n",
        "embedding_test[:1], conv_1d_output[:1], max_pool_output[:1]"
      ],
      "metadata": {
        "id": "eCOxpRIRWsqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed and create embedding layer(new embedding layer for each model)\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow.keras import layers\n",
        "model_5_embedding = layers.Embedding(input_dim = max_vocab_length,\n",
        "                                     output_dim = 128,\n",
        "                                     embddings_initializer = \"uniform\",\n",
        "                                     input_length = max_length,\n",
        "                                     name=\"embedding_5\"\n",
        "                                     )\n",
        "# Create 1-dimensional convolutional layer to model sequences\n",
        "from tensorflow.keras import lyaers\n",
        "inputs = layers.input(shape=(1,), dtype = \"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = model_5_embedding(x)\n",
        "x = layers.Conv1D(filters = 32, kernel_size = 5, activation=\"relu\")\n",
        "x = layers.GlobalMaxPool1D(x)\n",
        "# x = layers.Dense(64, activation = \"relu\")(x) # optional dense layer\n",
        "outputs = layers.Dense(1, activation = \"sigmoid\")(x)\n",
        "model_5 = tf.keras.Model(inputs, outputs, name = \"model_5_Conv1D\")\n",
        "\n",
        "# Compile Conv1D model\n",
        "model_5.compile(loss=\"binary_crossentorpy\",\n",
        "                optimizer =tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Get a summary of our 1D convolution model\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "fPKl4BgbXEEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_5_historly = model_5.fit(train-setences,\n",
        "                               train_labels,\n",
        "                               epochs = 5,\n",
        "                               validation_data = (vsl_setneces, val_labels),\n",
        "                               callbacks=[create_tensoroard_callback(SAVE_DIR,\n",
        "                                                                     \"Conv1D\")])"
      ],
      "metadata": {
        "id": "dCgDyYp7YqoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mke prediction with model_5\n",
        "model_5_pred_probs =  model_5.predict(val_senteces)\n",
        "model_5_pred_probs[:10]"
      ],
      "metadata": {
        "id": "2UsmOypnmKE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert model_5 predition probabilites to labels\n",
        "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
        "model_5_preds[:10]"
      ],
      "metadata": {
        "id": "OhlxMqDYmTb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model_5 evaluation metrics\n",
        "model_5_results = calcuate_result(y_true = val_labels,\n",
        "                                  y_pred = model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "id": "AEqdeYvNmd2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare model_5 results to baseline\n",
        "compare_baseline_to_new_results(baseline_results, mode_5_results)"
      ],
      "metadata": {
        "id": "c0SA0VCrmw89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usint Pretrained Embeddings (transfer learning for NLP)"
      ],
      "metadata": {
        "id": "wD5x-F5Tm9SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 6: TensorFlow Hub pretrained Setence Encoder"
      ],
      "metadata": {
        "id": "2MpU6aPAnCWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of pretrained embedding with universal sentece encoder\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\") # load univerwal sentece encoder\n",
        "embed_samples = embed([sample_sentece,\n",
        "                       \"When you call the univerwal sentece encoder on a sentence, it turns it into numbers\"])\n",
        "\n",
        "print(embed_smaples[0][:50])"
      ],
      "metadata": {
        "id": "OsznfTthnZko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each sentece has been encoded into a 512 dimentsion vector\n",
        "embed_samples[0].shape"
      ],
      "metadata": {
        "id": "zXT5AYP1oIKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use this encoding layer in place ot our text_vectorizer and embedding layer\n",
        "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shpae = [], # shape of inputs coming to our model\n",
        "                                        dtype = tf.string, # data type of inputs coming to the USE layer\n",
        "                                        trainable = False, # keep the pretrained weights (we'll create a feature extractor)\n",
        "                                        name=\"USE\")"
      ],
      "metadata": {
        "id": "1rfpfUvBoQ8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model using the Sequential API\n",
        "model_6 = tf.keras.Sequential([\n",
        "    sentece_encoder_layer, # take in senteces and then encode into an embedding\n",
        "    layers.Dense(64, activation = \"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_6_USE\")\n",
        "\n",
        "# Compile model\n",
        "model_6.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_6.summary()"
      ],
      "metadata": {
        "id": "ODKDlBWtpC0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a classifier on top or pretrained embeddings\n",
        "model_6_history = model_6.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs = 5,\n",
        "                              validation_data = (val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR,\n",
        "                                                                     \"tf_hub_sentence_encoder\")])"
      ],
      "metadata": {
        "id": "Ywub7f0tph4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with USE TF Hub model\n",
        "model_6_pred_probs = model_6.predict(val_senteces)\n",
        "model_6_pred_probs[:10]"
      ],
      "metadata": {
        "id": "MkwVHKKeqBz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction probabilites to labels\n",
        "model_6_pres = tf.squeeze(tf.round(model_6_pred_probs))\n",
        "model_6_preds[:10]"
      ],
      "metadata": {
        "id": "ZgJtjlZjqNRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculte model 6 performace metrics\n",
        "model_6_results = calculate_results(val_labels, model_6_preds)\n",
        "model_6_results"
      ],
      "metadata": {
        "id": "dRaLBCRsqVNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare TF Hub model to baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_6_results)"
      ],
      "metadata": {
        "id": "-EzeK5mWqh3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 7 : tensorflow Hub Pretrained Sentence Encoder 10% of the trainging data"
      ],
      "metadata": {
        "id": "uL3GhDYXqsD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### NOTE : Making splits like this will lead to data leakage\n",
        "### (some of the training examples in the validation set)\n",
        "\n",
        "# Create substes of 10% of the training data\n",
        "train_10_percent = train_df_shuffled[[\"text\",\"target\"]].sampel(frac=0.1, random_state = 42)\n",
        "train_sentences_10_percent = train_10_percent[\"text\"].to_list()\n",
        "train_labels_10_percnet = train_10_percent[\"target\"].to_list()\n",
        "len(train_sentences-10_percent), len(train_labels_10_percent)"
      ],
      "metadata": {
        "id": "nn3pDJwpv_Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One kind of correct way (there are more) to make data subset\n",
        "# (split the already split train_sentences/train_labels)\n",
        "train_senteces_90_percent, train_sentences_10_percent, train_labels_90_percent, train_labels_10_percent = train_test_split(np.array(train_setences),\n",
        "                                                                                                                           train_labels,\n",
        "                                                                                                                           test_size = 0.1,\n",
        "                                                                                                                           random_state = 42)"
      ],
      "metadata": {
        "id": "ULFUbGOdw4hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check length of 10 percent datasets\n",
        "print(f\"Total training examples:{len(train_sentences)}\")\n",
        "print(f\"Length of 10% training examples :{len(train_sentences_10_percent)}\")"
      ],
      "metadata": {
        "id": "8vL1C43-xTL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of targets in our subset of data\n",
        "# (this should be close to the distribution of labels int he original train_labels)\n",
        "pd.Series(train_labels_10_percent).value_counts()"
      ],
      "metadata": {
        "id": "eL9x4psjxgsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone model_6 buty reset weights\n",
        "model_7 = tf.keras.models.clone_model(model_6)\n",
        "\n",
        "# compile model\n",
        "model_7.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Get a summary (will be same as model_6)\n",
        "model_7.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "id": "fRccabAOxyQf",
        "outputId": "2fc5af1e-56c8-46b9-8565-e3a3fb5c4b8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d32e6067236c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# clone model_6 buty reset weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model_7.compile(loss=\"binary_crossentropy\",\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to 10% of the training data\n",
        "model_7_history = model_7.fit(x=train_sentences_10_percnet,\n",
        "                              y=train_labels_10_percent,\n",
        "                              epochs = 5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks = [create_tensorboard_callback(SAVE_DIR,\n",
        "                                                                       \"10_percent_tf_hub_sentece_encoder\")])"
      ],
      "metadata": {
        "id": "JjF4K_1yyNCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with the model trained on 10% of the data\n",
        "model_7_pred_probs = model_7.predict(val_senteces)\n",
        "mdoe_7_pred_probs[:10]"
      ],
      "metadata": {
        "id": "JvFocvUhyq6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction probabilities to labels\n",
        "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
        "model_7_preds[:10]"
      ],
      "metadata": {
        "id": "8LTcTLBJy1Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate model results\n",
        "model_7_results = calculate_results(val_lavels,model_7_preds)\n",
        "model_7_results"
      ],
      "metadata": {
        "id": "me9gp23uzATM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare to baseline\n",
        "compare_baseline_to_new_results(vaseline_results, model_7_results)"
      ],
      "metadata": {
        "id": "Fs6pSusuwTr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the performance of each of our models"
      ],
      "metadata": {
        "id": "-K9--9dhzTXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine model results into a DataFrame\n",
        "all_model_results = pd.DataFrame({\n",
        "    \"baseline\" : baseline_results,\n",
        "    \"simple_dense\": model_1_results,\n",
        "    \"lstm\" : model_2_results,\n",
        "    \"gru\" : model_3_results,\n",
        "    \"bidirectional\" : model_4_results,\n",
        "    \"conv1d\" : model_5_results,\n",
        "    \"tf_hub_sentecne_encoder\": model_6_reuslts,\n",
        "    \"tf_hub_10_percent-data\": model_6_results\n",
        "})\n",
        "\n",
        "all_model_results = all_model_results,transpose()\n",
        "all_model_results"
      ],
      "metadata": {
        "id": "Cl85wR6FzdRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the accuracy to same scale as other metrics\n",
        "all_model_results[\"accuracy\"] = all_model-results[\"accuracy\"]/100"
      ],
      "metadata": {
        "id": "QYwiBs9-0M_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and compare all of the model results\n",
        "all_model_results.plot(kind=\"bar\", figsize = (10,7)).legend(bbox_to_anchor=(1.0, 1.0));"
      ],
      "metadata": {
        "id": "99jKttiD0bEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort model results by f1-score\n",
        "all_model_results.sort_values('f1', ascending=False)[\"f1\"].plot(kind=\"bar\", figsize = (10, 7))"
      ],
      "metadata": {
        "id": "drSrGffD0r56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get mean pred probs for 3 models\n",
        "baseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis = 1) # get the prediction probabilities form baseline model\n",
        "combined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis = 1) + tf.squeeze(model_6_pred_probs)\n",
        "combined_preds = tf.round(combined_pred_probs/3) # average and ound the prediction probabilities to get predicion classes\n",
        "combined_preda[:20]"
      ],
      "metadata": {
        "id": "SJWoQrhS08jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate results form averaging the prediction probabilites\n",
        "ensemble_results = calculate_results(val_labels, combined_preds)\n",
        "ensemble_results"
      ],
      "metadata": {
        "id": "Av1lWXna2SRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add our combined model's results to the results DataFrame\n",
        "all_model_reulsts.loc[\"ensemble_results\"] = ensemble_results"
      ],
      "metadata": {
        "id": "fiqovGbC2ebj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add our combiend model's results to the results DataFrame\n",
        "all_model-results.loc[\"ensemble_results\"] = ensemble_results"
      ],
      "metadata": {
        "id": "c27CdBWl2odp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the accuracy to the same scale as the rest of the results\n",
        "all_model_results.loc[\"ensemble_resulst\"][\"accuracy\"] = all_model_results.loc[\"ensemble_results\"][\"accuracy\"]/100"
      ],
      "metadata": {
        "id": "KLI0grU-2wBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_results"
      ],
      "metadata": {
        "id": "z9cahaeR3CQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and loading a trained model\n"
      ],
      "metadata": {
        "id": "Zwf1wRB53Edq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save TF Hub Sentence Encoder model to HDF5 format\n",
        "model_6.save(\"model_6.h5\")"
      ],
      "metadata": {
        "id": "_SySKT233Uil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lode model with custom Hub layer (required with HDF5 format)\n",
        "loaded_model_6 =tf.keras.models.load_model(\"model_6.h5\",\n",
        "                                           custom_object={\"KerasLayer\":hub.KerasLayer})"
      ],
      "metadata": {
        "id": "UtE5RrVi4_9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How does our loaded model perform?\n",
        "loaded_model_6.eZvaluate(val_senteces, val_labels)"
      ],
      "metadata": {
        "id": "tX6Nfh6D5Y00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save TF Hub Sentences Encoder model to SaveModel format (default)\n",
        "model_6.save(\"model_6_SaveModel_format\")"
      ],
      "metadata": {
        "id": "7CUbhryP5g6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TF Hub Sentence Encoder SaveModel\n",
        "loaded_model_6_SavedModel = tf.keras.models.load_model(\"model_6_SavedModel_format\")"
      ],
      "metadata": {
        "id": "hG4fenBH5rOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded SavedModel format\n",
        "loaded_model_6_SavedModel.evaluate(val_setences, val_labels)"
      ],
      "metadata": {
        "id": "BhACz0Th52lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding the most wrong examples"
      ],
      "metadata": {
        "id": "LxsApt4K6B61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframe with validation senteces and best performing model predictions\n",
        "val_df = pd.Dataframe({'text': val_sentences,\n",
        "                       \"target\": val_labels,\n",
        "                       \"pred\" : model_6_preds,\n",
        "                       \"pred_prob\" :tf.squeeze(model_6_pred_probs)})"
      ],
      "metadata": {
        "id": "bRKV0Mn26FVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the wrong predictions and sort by predictions probabilities\n",
        "most_wrong = val_df[val_dr[\"target\"] != val_dr[\"pred\"]].sort_values(\"pred_prob\", ascending = False)\n",
        "most_wrong[:10]"
      ],
      "metadata": {
        "id": "OS8YGMef6t1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the false positives (model predicted 1 when should've been 0)\n",
        "for row inmost_wrong[:10].itertuples(): # loop though the top 10 rows (change the index to view different rows)\n",
        "    _, text, target, pred, prob = row\n",
        "    print(f\"Target : {target}, Pred:{int(pred)}, Prob : {prob}\")\n",
        "    print(f\"Text:\\n{text}\\n\")\n",
        "    print(\"---\\n\")"
      ],
      "metadata": {
        "id": "u6fwQqms7BJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the most wrong false nagatives (model predicted 0 when should've predict 1)\n",
        "for row in most_wrong[-10:].itertuples():\n",
        "    _, text, target, pred, prob = row\n",
        "    print(f\"Target :{target}, Pred : {int(pred)}, Prob :{prob}\")\n",
        "    print(f\"Text :\\n {text}\\n\")\n",
        "    print(\"------\\n\")"
      ],
      "metadata": {
        "id": "ehkL3hR27sMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making predictions on the test dataset"
      ],
      "metadata": {
        "id": "5dHuEVJP8LRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions on the test dataset\n",
        "test_sentecnes = test_df[\"text\"].to_list()\n",
        "test_samples = random.sample(test_sentences, 10)\n",
        "for test_sample in test_sampels:\n",
        "    pred_prob = tf.queeze(model_6.predict([test_sample]))# has to be list\n",
        "    pred = tf.round(pred_prob)\n",
        "    print = tf.round(pred_prob)\n",
        "    print(f\"Pred : {int(pred)}, Prob:{pred_prob}\")\n",
        "    print(f\"Text:\\n{test_sample}\\n\")\n",
        "    print(\"-----\\n\")"
      ],
      "metadata": {
        "id": "6EjE6UTw8PD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting on Tweets from the wild"
      ],
      "metadata": {
        "id": "quDslPBq89Au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn tweet into string\n",
        "daniels_tweet = \"Life lie an esemble: take the best choices form others and make your own\""
      ],
      "metadata": {
        "id": "747A0Bv89Gqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on-sentece(model, sentecs):\n",
        "    pred_probs = model.predict([sentence])\n",
        "    pred_label = tf.squeeze(tf.round(pred_prob)).numpy()\n",
        "    print(f\"Pred : {pred_label}\", \"(real disaster)\" if pred_label > 0 else \"(not real disaster)\", f\"Prob: {pred_prob[0][0]}\")\n",
        "    print(f\"Text:\\n{sentence}\")"
      ],
      "metadata": {
        "id": "sSZrGdnoCv0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction on Tweet form the wild\n",
        "prediction_on_sentence(model=model_6, # use the USE model\n",
        "                       sentence=dainels_tweet)"
      ],
      "metadata": {
        "id": "xTerD1J1DSBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beirut_tweet_1 = \"Reports that the smoke in Beirut sky contain nitric acid, which is toxic. Plase share the refrain from stepping outside unless urgent. #Lebanon\"\n",
        "beirut_tweet_2 = \"#Beirut decalred a 'devastated city', Two-week state of emergency offically declared. #Lebanon\""
      ],
      "metadata": {
        "id": "dlgG2rdmDdDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on diaster Tweet 1\n",
        "predict_on_sentece(model=model_6,\n",
        "                   sentece=beirut_tweet_1)"
      ],
      "metadata": {
        "id": "iRa9VCoGEIcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on disaster Tweet 2\n",
        "predict_on_setnece(model=model_6,\n",
        "                   sentecne=beirut_tweet_2)\n"
      ],
      "metadata": {
        "id": "UN1JiudKEPUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The speed/score tradeoff"
      ],
      "metadata": {
        "id": "bEo_mCS4EZFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the time of predictions\n",
        "import itme\n",
        "def pred_timer(model, smaples):\n",
        "    start_time = time.perf_counter() # get start time\n",
        "    model.predict(smaples) # make predictions\n",
        "    end_time = time.perf_counter() # get finish time\n",
        "    total_time = end_time-start_time # calculate how long predictions took to make\n",
        "    time_per_pred = total_time/len(val_sentences)# find prediction time per sampel\n",
        "    return total_time, time_per_pred"
      ],
      "metadata": {
        "id": "m1zoegpGEdFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcultate TF Hub Sentence Encoder prediction times\n",
        "model_6_total_pred_time, model_6_time_per_pred = pred_timer(model_6, val_sentences)\n",
        "model_6_total_pred_time, model_6_time_per_pred"
      ],
      "metadata": {
        "id": "lnZh2rkcFYH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Naive Bayes prediction times\n",
        "baseline_total_pred_time, baseline_time_per_pred = pred_timer(model_0, val_sentences)\n",
        "baseline_total_pre_time, baseline_time_per_pred"
      ],
      "metadata": {
        "id": "Tbl5HkTSFrUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (10, 7))\n",
        "plt.scatter(baseline_time_pred, baseline_result[\"f1\"], label=\"baseline\")\n",
        "plt.scatter(model_6_time_per_pred, model_6_reuslts[\"f1\"], label=\"tf_hub_setence_encoder\")\n",
        "plt.legend()\n",
        "plt.title(\"F1-scroe versus time per prediction\")\n",
        "plt.xlabel(\"Time per prediction\")\n",
        "plt.ylabel(\"F1-score\")"
      ],
      "metadata": {
        "id": "kCSaEfygF-3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HZ9iSOWGGhGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SIopdvgk5_pM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}